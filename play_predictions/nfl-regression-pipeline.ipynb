{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "warnings.simplefilter('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore',category=ImportWarning)\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import randint, loguniform\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, HalvingRandomSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:32.860495Z",
     "start_time": "2023-12-22T17:36:29.767337Z"
    }
   },
   "id": "774348c602e5ca1e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Train test split parameters\n",
    "test_holdout_percentage = 0.30\n",
    "\n",
    "# Leave One Out Encoder Sigma value - 0.04 is the top performing value.\n",
    "sigma = 0.04                                                        # Tested parameters: 0.04, 0.05, 0.10, 0.30, 0.60\n",
    "\n",
    "# Feature scaler\n",
    "feature_scaler = StandardScaler()                                    # Tested parameters: MinMaxScaler(), StandardScaler(), MaxAbsScaler(), RobustScaler()\n",
    "\n",
    "# HalvingRandomSearchCV parameters\n",
    "scoring = 'neg_mean_squared_error'\n",
    "n_cross_validation = 3\n",
    "\n",
    "# Specify the HalvingRandomSearchCV parameters\n",
    "halving_parameter = 2.0\n",
    "max_resource = 500\n",
    "resource_divisor = 2.0\n",
    "min_resource = int(round((max_resource / resource_divisor), 0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:32.871766Z",
     "start_time": "2023-12-22T17:36:32.864046Z"
    }
   },
   "id": "39c59abc33e5e515"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Create timer to calculate total workbook time in hours\n",
    "start_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:32.873708Z",
     "start_time": "2023-12-22T17:36:32.869806Z"
    }
   },
   "id": "5dc8446203dc414e"
  },
  {
   "cell_type": "markdown",
   "id": "c4f8e047",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## IMPORT PROCESSED NFL-DATA-PY CSV FILE\n",
    "##### https://pypi.org/project/nfl-data-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(60331, 160)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import csv file from nfl-data-py\n",
    "df = pd.read_csv(r'/Users/ttas2/Documents/Python/nfl-machine-learning-models/output_files/nfl_post_processing_run_pass_classification_data.csv')\n",
    "\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:34.285592Z",
     "start_time": "2023-12-22T17:36:32.878648Z"
    }
   },
   "id": "f8d686327dce3e32"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Print columns with missing values\n",
    "print(df.columns[df.isnull().any()].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:34.343493Z",
     "start_time": "2023-12-22T17:36:34.279119Z"
    }
   },
   "id": "629bc39586b0fba6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "       week posteam posteam_type defteam  yardline_100   \n19450    14     DET         home     MIN          56.0  \\\n935      16     ARI         home      TB          91.0   \n\n       quarter_seconds_remaining  half_seconds_remaining   \n19450                        481                     481  \\\n935                          659                    1559   \n\n       game_seconds_remaining  game_half  qtr  ...   \n19450                     481          2    4  ...  \\\n935                      1559          2    3  ...   \n\n       remain_yds_div_def_dl_count  remain_yds_prod_def_dl_count   \n19450                     3.000000                      3.000000  \\\n935                       3.166667                     12.666667   \n\n       remain_yds_div_def_db_count remain_yds_prod_def_db_count   \n19450                     0.600000                    15.000000  \\\n935                       1.266667                    31.666667   \n\n       remain_yds_div_score_diff  remain_yds_prod_score_diff   \n19450                       0.25                        36.0  \\\n935                         0.00                         0.0   \n\n       run_ratio_off_priors  run_ratio_def_priors  posteam_season   \n19450              0.194444              0.166667        det_2022  \\\n935                0.585714              0.528571        ari_2022   \n\n       defteam_season  \n19450        min_2022  \n935           tb_2022  \n\n[2 rows x 160 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>week</th>\n      <th>posteam</th>\n      <th>posteam_type</th>\n      <th>defteam</th>\n      <th>yardline_100</th>\n      <th>quarter_seconds_remaining</th>\n      <th>half_seconds_remaining</th>\n      <th>game_seconds_remaining</th>\n      <th>game_half</th>\n      <th>qtr</th>\n      <th>...</th>\n      <th>remain_yds_div_def_dl_count</th>\n      <th>remain_yds_prod_def_dl_count</th>\n      <th>remain_yds_div_def_db_count</th>\n      <th>remain_yds_prod_def_db_count</th>\n      <th>remain_yds_div_score_diff</th>\n      <th>remain_yds_prod_score_diff</th>\n      <th>run_ratio_off_priors</th>\n      <th>run_ratio_def_priors</th>\n      <th>posteam_season</th>\n      <th>defteam_season</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19450</th>\n      <td>14</td>\n      <td>DET</td>\n      <td>home</td>\n      <td>MIN</td>\n      <td>56.0</td>\n      <td>481</td>\n      <td>481</td>\n      <td>481</td>\n      <td>2</td>\n      <td>4</td>\n      <td>...</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>0.600000</td>\n      <td>15.000000</td>\n      <td>0.25</td>\n      <td>36.0</td>\n      <td>0.194444</td>\n      <td>0.166667</td>\n      <td>det_2022</td>\n      <td>min_2022</td>\n    </tr>\n    <tr>\n      <th>935</th>\n      <td>16</td>\n      <td>ARI</td>\n      <td>home</td>\n      <td>TB</td>\n      <td>91.0</td>\n      <td>659</td>\n      <td>1559</td>\n      <td>1559</td>\n      <td>2</td>\n      <td>3</td>\n      <td>...</td>\n      <td>3.166667</td>\n      <td>12.666667</td>\n      <td>1.266667</td>\n      <td>31.666667</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.585714</td>\n      <td>0.528571</td>\n      <td>ari_2022</td>\n      <td>tb_2022</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 160 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert binary columns to integers\n",
    "binary_columns = df.columns[df.isin([0,1]).all()].tolist()\n",
    "df[binary_columns] = df[binary_columns].apply(pd.to_numeric, downcast='integer', errors='coerce', axis=1)\n",
    "\n",
    "df.sample(2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:44.729282Z",
     "start_time": "2023-12-22T17:36:34.335662Z"
    }
   },
   "id": "41f7422bcc2cf003"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c9bcb28",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:44.743076Z",
     "start_time": "2023-12-22T17:36:44.721684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "play_type\npass    0.598382\nrun     0.401618\nName: proportion, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target frequency\n",
    "target_count = df.play_type.value_counts(normalize=True)\n",
    "\n",
    "target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df['play_type'] = np.where(df['play_type'] == 'pass', 1, 0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:44.757723Z",
     "start_time": "2023-12-22T17:36:44.734763Z"
    }
   },
   "id": "91112ef9f7a1ba3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TRAIN TEST SPLIT\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c140526b778c182"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4943bbf5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:44.929611Z",
     "start_time": "2023-12-22T17:36:44.745650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 initial features before processing\n"
     ]
    }
   ],
   "source": [
    "# split data into target and feature datasets\n",
    "X, y = df.loc[:, df.columns != 'play_type'], df['play_type']\n",
    "\n",
    "initial_features = X.columns.to_list()\n",
    "\n",
    "# Create train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_holdout_percentage, random_state=67)\n",
    "\n",
    "# Specify number of target classes\n",
    "n_classes = round(y_train.nunique() * adasyn_class_multiplier, 0)\n",
    "\n",
    "# Used to balancing the effect of XGBClassifier weights on imbalanced dataset: scale_pos_weight = total_majority observations / total minority observations\n",
    "target_count = y_train.value_counts()\n",
    "scale_pos_weight = round(target_count[1] / target_count[0], 2)\n",
    "\n",
    "print('XGBClassifier scale_pos_weight:', scale_pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cd4c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BASELINE MODEL\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28e9513e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:44.930468Z",
     "start_time": "2023-12-22T17:36:44.910611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 59.8%\n",
      "Baseline f1 score: 74.9%\n"
     ]
    }
   ],
   "source": [
    "# Create and fit baseline model to compare performance\n",
    "baseline_model = DummyRegressor(strategy='mean', )\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate model accuracy on test data\n",
    "y_baseline_pred = baseline_model.predict(X_test)\n",
    "\n",
    "print(f\"Baseline accuracy: {round(mean_squared_error(y_test, y_baseline_pred)*100, 1)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MODEL PIPELINE\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html\n",
    "##### https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html\n",
    "##### https://imbalanced-learn.org/stable/references/over_sampling.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "##### https://xgboost.readthedocs.io/en/stable/parameter.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28eb7c49ea06b6d2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordinal features: 4 : ['game_half', 'qtr', 'down', 'remaining_downs']\n",
      " \n",
      "boolean features: 66 : ['goal_to_go', 'shotgun', 'no_huddle', 'div_game', 'report_eligible', 'dtg_99to90', 'dtg_89to60', 'dtg_59to45', 'dtg_44to35', 'dtg_34to21', 'dtg_20to10', 'dtg_09to00', 'prev1_big_play_pass', 'prev2_big_play_pass', 'prev3_big_play_pass', 'prev1_big_play_run', 'prev2_big_play_run', 'prev3_big_play_run', 'prev1_negative_pass', 'prev2_negative_pass', 'prev3_negative_pass', 'prev1_negative_run', 'prev2_negative_run', 'prev3_negative_run', 'prev1_play_off_penalty', 'prev1_play_def_penalty', 'prev2_play_off_penalty', 'prev2_play_def_penalty', 'prev3_play_off_penalty', 'prev3_play_def_penalty', 'prev1_play_run_outside', 'prev1_play_run_inside', 'prev1_play_pass_deep', 'prev1_play_pass_short', 'prev2_play_run_outside', 'prev2_play_run_inside', 'prev2_play_pass_deep', 'prev2_play_pass_short', 'prev3_play_run_outside', 'prev3_play_run_inside', 'prev3_play_pass_deep', 'prev3_play_pass_short', 'prev1_incomplete_pass', 'prev2_incomplete_pass', 'prev3_incomplete_pass', 'prev4_incomplete_pass', 'prev1_shotgun', 'prev2_shotgun', 'prev3_shotgun', 'prev1_qb_hit', 'prev2_qb_hit', 'prev3_qb_hit', 'prev1_no_huddle', 'prev2_no_huddle', 'prev3_no_huddle', 'prev1_first_down_pass', 'prev2_first_down_pass', 'prev3_first_down_pass', 'prev1_first_down_run', 'prev2_first_down_run', 'prev3_first_down_run', 'prev1_effct_play', 'prev2_effct_play', 'prev3_effct_play', 'two_min_warning', 'posteam_side']\n",
      " \n",
      "numeric features: 73 : ['remain_yds_prod_def_dl_count', 'off_rb_count', 'td_prob', 'run_ratio_off_priors', 'hb_to_lb_ratio', 'remaining_yards_per_down', 'off_te_count', 'no_score_prob', 'drive_no_huddle_pcnt', 'wp', 'half_seconds_remaining', 'half_seconds_div_score_diff', 'wr_to_db_ratio', 'fg_prob', 'score_differential', 'game_temp_div_game_humidity', 'game_wind_prod_game_temp', 'drive_play_count', 'posteam_timeouts_remaining', 'ydstogo', 'game_temp_prod_game_humidity', 'ep', 'drive_incomplete_pass_pcnt', 'run_ratio_def_priors', 'prev1_yards_gained', 'safety_prob', 'remain_yds_div_off_hb_count', 'def_db_count', 'quarter_seconds_remaining', 'off_hb_count', 'prev4_wpa', 'defteam_score', 'week', 'ep_half_sec_ratio', 'remain_yds_div_def_db_count', 'drive_qb_hit_pcnt', 'game_wind_div_game_temp', 'remain_yds_prod_def_db_count', 'prev3_yards_gained', 'play_sequence_game', 'prev4_yards_gained', 'play_sequence_series', 'off_ol_count', 'defteam_timeouts_remaining', 'prev2_wpa', 'game_temp', 'yardline_100', 'prev1_wpa', 'remain_yds_div_def_box', 'spread_line', 'remain_yds_prod_score_diff', 'prev3_wpa', 'total_line', 'ep_game_sec_ratio', 'remain_yds_prod_off_hb_count', 'off_wr_count', 'remain_yds_prod_def_box', 'remain_yds_div_def_dl_count', 'game_seconds_remaining', 'posteam_score', 'drive_effct_play_pcnt', 'def_dl_count', 'n_offense', 'n_defense', 'drive_shotgun_pcnt', 'ol_to_dl_ratio', 'prev2_yards_gained', 'game_humidity', 'game_wind', 'def_lb_count', 'half_seconds_prod_score_diff', 'defenders_in_box', 'remain_yds_div_score_diff']\n",
      " \n",
      "categorical features: 16 : ['posteam', 'posteam_type', 'defteam', 'roof', 'surface', 'offense_formation', 'offense_personnel', 'defense_personnel', 'game_weather', 'play_type_prev1', 'play_type_prev2', 'play_type_prev3', 'play_type_prev4', 'drive_start', 'posteam_season', 'defteam_season']\n",
      " \n",
      "feature count: 159\n"
     ]
    }
   ],
   "source": [
    "# Create feature type lists for column transform stage of the pipeline\n",
    "ordinal_features = X_train.columns[X_train.isin([1,2,3,4,5]).all()].tolist()\n",
    "categorical_features = list(X_train.select_dtypes(include='object'))\n",
    "boolean_features = X_train.columns[X_train.isin([0, 1]).all()].tolist()\n",
    "\n",
    "# define numeric features as remaining features not in ordinal categorical or boolean lists\n",
    "numeric_features = list(set(X_train.columns) - set(ordinal_features) - set(categorical_features) - set(boolean_features))\n",
    "\n",
    "#print('categorical features:', len(categorical_features), ':', categorical_features)\n",
    "print('ordinal features:', len(ordinal_features), ':', ordinal_features)\n",
    "print(' ')\n",
    "print('boolean features:', len(boolean_features), ':', boolean_features)\n",
    "print(' ')\n",
    "print('numeric features:', len(numeric_features), ':', numeric_features)\n",
    "print(' ')\n",
    "print('categorical features:', len(categorical_features), ':', categorical_features)\n",
    "print(' ')\n",
    "print('feature count:', len(initial_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:46.661426Z",
     "start_time": "2023-12-22T17:36:44.975063Z"
    }
   },
   "id": "e95804b4c341fcbb"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Custom transformer for IQR outlier exclusion\n",
    "class IQRTransformer:\n",
    "    def __init__(self, numerical_cols):\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.lower_bound = None\n",
    "        self.upper_bound = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Calculate the IQR for each numerical column\n",
    "            q1 = X[self.numerical_cols].quantile(0.25)\n",
    "            q3 = X[self.numerical_cols].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "\n",
    "            # Define the lower and upper bounds for outliers\n",
    "            self.lower_bound = (q1 - 1.5 * iqr).to_dict()\n",
    "            self.upper_bound = (q3 + 1.5 * iqr).to_dict()\n",
    "        else:\n",
    "            # Calculate the IQR for each numerical column\n",
    "            q1 = np.quantile(X[:, :], 0.25, axis=0)\n",
    "            q3 = np.quantile(X[:, :], 0.75, axis=0)\n",
    "            iqr = q3 - q1\n",
    "\n",
    "            # Define the lower and upper bounds for outliers\n",
    "            self.lower_bound = (q1 - 1.5 * iqr).tolist()\n",
    "            self.upper_bound = (q3 + 1.5 * iqr).tolist()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Exclude outliers based on the IQR for each numerical column\n",
    "            x_outlier_removed = X.copy()\n",
    "            for col in self.numerical_cols:\n",
    "                if col in self.lower_bound and col in self.upper_bound:\n",
    "                    x_outlier_removed = x_outlier_removed[\n",
    "                        (x_outlier_removed[col] >= self.lower_bound[col]) & (x_outlier_removed[col] <= self.upper_bound[col])\n",
    "                    ].dropna()\n",
    "        else:\n",
    "            # Exclude outliers based on the IQR for each numerical column\n",
    "            x_outlier_removed = X.copy()\n",
    "            for i, col in enumerate(self.numerical_cols):\n",
    "                if col in self.lower_bound and col in self.upper_bound:\n",
    "                    lower_bound = self.lower_bound[col]\n",
    "                    upper_bound = self.upper_bound[col]\n",
    "                    x_outlier_removed = x_outlier_removed[\n",
    "                        (x_outlier_removed[:, i] >= lower_bound) & (x_outlier_removed[:, i] <= upper_bound)\n",
    "                    ].dropna()\n",
    "\n",
    "        return x_outlier_removed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T17:36:46.664544Z",
     "start_time": "2023-12-22T17:36:46.661600Z"
    }
   },
   "id": "716f61f917c8a815"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 250\n",
      "max_resources_: 500\n",
      "aggressive_elimination: False\n",
      "factor: 2.0\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 2\n",
      "n_resources: 250\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    }
   ],
   "source": [
    "# Specify the transformations per data type\n",
    "num_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "                            ('iqr_outlier', IQRTransformer(numerical_cols=numeric_features)),\n",
    "                            ('scaler', feature_scaler),\n",
    "                           ])\n",
    "\n",
    "cat_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                            ('cat_encoder', LeaveOneOutEncoder(handle_missing='value', handle_unknown='value', sigma=sigma, random_state=67)),\n",
    "                            ])\n",
    "\n",
    "ord_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                            ('ordinal_encoder', LeaveOneOutEncoder(handle_missing='value', handle_unknown='value', sigma=sigma, random_state=67)),\n",
    "                           ])\n",
    "\n",
    "preprocessing = ColumnTransformer(transformers=[('numeric_transform', num_trans, numeric_features),\n",
    "                                                ('categorical_transform', cat_trans, categorical_features),\n",
    "                                                ('ordinal_transform', ord_trans, ordinal_features),\n",
    "                                                ],\n",
    "                                     remainder='passthrough',\n",
    "                                    )\n",
    "\n",
    "# Define the models\n",
    "models = [\n",
    "    ('RandomForest', RandomForestRegressor()),\n",
    "    ('ExtraTrees', ExtraTreesRegressor()),\n",
    "    ('GradientBoosting', GradientBoostingRegressor()),\n",
    "    ('AdaBoost', AdaBoostRegressor()),\n",
    "    ('XGBoost', XGBRegressor()),\n",
    "]\n",
    "\n",
    "# Create and run the pipeline\n",
    "for model_name, model in models:\n",
    "    pipeline = Pipeline([\n",
    "        ('pre', preprocessing),\n",
    "        ('select', SelectKBest()),\n",
    "        ('smpl', ADASYN(sampling_strategy='not majority', random_state=67)),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    if model_name == 'RandomForest':\n",
    "        params = {\n",
    "            'select__k': randint(10, 80),\n",
    "            'smpl__n_neighbors': randint(2, 15),                       # Only for sampling_strategy='not majority\n",
    "            'clf__bootstrap': [True],\n",
    "            'clf__ccp_alpha': loguniform(1e-06, 1e-01),   # cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting\n",
    "            'clf__criterion': ['gini','entropy'],\n",
    "            'clf__max_depth': randint(5, 30),\n",
    "            'clf__max_features': loguniform(0.10, 0.35), \n",
    "            'clf__min_impurity_decrease': loguniform(1e-09, 1e-04),\n",
    "            #'clf__max_samples': loguniform(0.02, 0.49),              # Only for bootstrap=True\n",
    "            'clf__min_samples_leaf': loguniform(0.005, 0.20),\n",
    "            'clf__min_samples_split': loguniform(0.005, 0.20),\n",
    "            'clf__min_weight_fraction_leaf': loguniform(0.005, 0.20),\n",
    "            'clf__oob_score': [True, False],                                # Only for bootstrap=True\n",
    "            'clf__warm_start': [True, False],\n",
    "            'clf__n_jobs': [6],\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "    \n",
    "    elif model_name == 'ExtraTrees':\n",
    "        params = {\n",
    "            'select__k': randint(10, 80),\n",
    "            'smpl__n_neighbors': randint(2, 15),                       # Only for sampling_strategy='not majority\n",
    "            'clf__bootstrap': [False],\n",
    "            'clf__ccp_alpha': loguniform(1e-06, 1e-01),               # cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting\n",
    "            'clf__criterion': ['gini','entropy'],\n",
    "            'clf__max_depth': randint(5, 80),\n",
    "            'clf__max_features': loguniform(0.50, 0.95),\n",
    "            'clf__max_leaf_nodes': randint(20, 70),\n",
    "            #'clf__max_samples': loguniform(0.10, 0.50),               # Only for bootstrap=True\n",
    "            'clf__min_impurity_decrease': loguniform(1e-05, 1e-01),\n",
    "            'clf__min_samples_leaf': loguniform(0.05, 0.30),\n",
    "            'clf__min_samples_split': loguniform(0.005, 0.15),\n",
    "            'clf__min_weight_fraction_leaf': loguniform(0.05, 0.25),\n",
    "            'clf__oob_score': [False],                                # Only for bootstrap=True\n",
    "            'clf__warm_start': [True, False],\n",
    "            'clf__n_jobs': [6],\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "    \n",
    "    elif model_name == 'GradientBoosting':\n",
    "        params = {\n",
    "            'select__k': randint(10, 80),\n",
    "            'smpl__n_neighbors': randint(2, 15),                       # Only for sampling_strategy='not majority\n",
    "            'clf__criterion': ['friedman_mse'],\n",
    "            'clf__ccp_alpha': loguniform(1e-06, 1e-01),  # cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting\n",
    "            'clf__learning_rate': loguniform(1e-05, 1e-00),\n",
    "            'clf__loss': ['log_loss','exponential'],\n",
    "            'clf__max_depth': randint(25, 60),\n",
    "            'clf__max_features': loguniform(0.45, 0.85), \n",
    "            'clf__max_leaf_nodes': randint(20, 50),\n",
    "            'clf__min_weight_fraction_leaf': loguniform(0.30, 0.50),   # Must be <= 0.5\n",
    "            'clf__min_impurity_decrease': loguniform(1e-08, 1e-04),\n",
    "            'clf__min_samples_leaf': loguniform(0.01, 0.25),\n",
    "            'clf__min_samples_split': loguniform(0.10, 0.35),\n",
    "            'clf__n_iter_no_change': [200],\n",
    "            'clf__tol': loguniform(1e-08, 1e-03),\n",
    "            'clf__validation_fraction': loguniform(0.05, 0.15),\n",
    "            'clf__warm_start': [True, False],\n",
    "            'clf__subsample': loguniform(0.65, 1.0),\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "        \n",
    "    elif model_name == 'AdaBoost':\n",
    "        params = {\n",
    "            'select__k': randint(10, 80),\n",
    "            'smpl__n_neighbors': randint(2, 15),                       # Only for sampling_strategy='not majority\n",
    "            'clf__algorithm': ['SAMME','SAMME.R'],\n",
    "            'clf__learning_rate': loguniform(1e-08, 1e-01),\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "    \n",
    "    elif model_name == 'XGBoost':\n",
    "        params = {\n",
    "            'select__k': randint(10, 80),\n",
    "            'smpl__n_neighbors': randint(2, 15),                       # Only for sampling_strategy='not majority\n",
    "            'clf__booster': ['gbtree','dart'],\n",
    "            'clf__max_depth': randint(4, 8),\n",
    "            'clf__grow_policy': ['depthwise','lossguide'],\n",
    "            'clf__objective': ['reg:squarederror'],\n",
    "            'clf__eval_metric': ['mape'],\n",
    "            'clf__seed': [67],\n",
    "        }\n",
    "    \n",
    "    search = HalvingRandomSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=params,\n",
    "        scoring=scoring,\n",
    "        factor=halving_parameter,\n",
    "        resource='clf__n_estimators',\n",
    "        n_candidates='exhaust',\n",
    "        min_resources=min_resource,\n",
    "        max_resources=max_resource,\n",
    "        aggressive_elimination=False,\n",
    "        return_train_score=True,\n",
    "        refit=True,\n",
    "        cv=n_cross_validation,\n",
    "        n_jobs=6,\n",
    "        error_score='raise',\n",
    "        random_state=67,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best performance for {model_name}: {-search.best_score_}\")     # Note the negative sign for mean squared error\n",
    "    print(f\"Best parameters: {search.best_params_}\")\n",
    "\n",
    "    print(\"\\n\")\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-22T17:36:46.690684Z"
    }
   },
   "id": "58cc5161"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate workbook processing time in hours\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print('Total HalvingRandomSearchCV runtime:', round(total_time / 3600, 2), 'hours')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d7e4b6574bc08686"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8b4ac4275dcfbe6"
  }
 ],
 "metadata": {
  "colab": {
   "name": "nfl_offensive_play_classification_v1.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
