{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad9b19",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time_calc\n",
    "from time import time\n",
    "\n",
    "import category_encoders as ce\n",
    "from scipy.stats import randint\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, HalvingRandomSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "import joblib\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import data from nfl-data-py\n",
    "##### https://pypi.org/project/nfl-data-py/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f0f19",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read csv with 5-years of nfl play-by-play data (2020-2021)\n",
    "data = pd.read_csv(r'/Users/ttas2/Documents/Python/nfl-machine-learning-models/play_predictions/output_files/nfl_post_processing_multiclass_play_classification_data.csv')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print columns with missing values\n",
    "print(df.columns[df.isnull().any()].tolist())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412d98d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert binary columns to integers\n",
    "binary_columns = df.columns[df.isin([0,1]).all()].tolist()\n",
    "df[binary_columns] = df[binary_columns].apply(pd.to_numeric, downcast='integer', errors='coerce', axis=1)\n",
    "\n",
    "df.sample(3).style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Target frequency\n",
    "target_count = df.play_type.value_counts(normalize=True)\n",
    "target_count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train test split data\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "play_type\nshort      0.412871\ninside     0.216227\noutside    0.196435\ndeep       0.174466\nName: proportion, dtype: float64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe365d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split data into target and feature datasets\n",
    "X, y = df.loc[:, df.columns != 'play_type'], df['play_type']\n",
    "\n",
    "# initial_features = df.drop(['play_type'], axis=1)\n",
    "initial_features = X.columns.to_list()\n",
    "\n",
    "# Create train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=67)\n",
    "\n",
    "print('Starting analysis with', X_train.shape[1], 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encode categorical features\n",
    "##### https://contrib.scikit-learn.org/category_encoders/leaveoneout.html"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T19:59:00.835606Z",
     "start_time": "2023-07-18T19:59:00.827235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 86\n",
      "Feature columns: 82\n",
      "Target columns: 4\n",
      "Target columns: ['play_type_deep', 'play_type_outside', 'play_type_short', 'play_type_inside']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create list of features for each dtype\n",
    "categorical_feat = list(X_train.select_dtypes(include='object'))\n",
    "\n",
    "# One hot encoding of categorical features\n",
    "# encoder = ce.OneHotEncoder(return_df=True, cols=categorical_feat, use_cat_names=True)\n",
    "\n",
    "# Encoded column represents mean response over all rows for this category, providing one-column representation while avoiding direct response leakage\n",
    "encoder = ce.LeaveOneOutEncoder(return_df=True, cols=categorical_feat)\n",
    "\n",
    "X_train = encoder.fit_transform(X_train, y_train)\n",
    "X_test = encoder.transform(X_test)\n",
    "\n",
    "print(X_train.shape[1], 'features after categorical encoding')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BorutaShap Feature selection\n",
    "##### https://pypi.org/project/BorutaShap/\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis with 82 features\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify feature selection model\n",
    "feature_model = GradientBoostingClassifier(loss='log_loss',\n",
    "                                           n_estimators=100,\n",
    "                                           max_depth=40,\n",
    "                                           max_features=0.4,\n",
    "                                           max_leaf_nodes=30,\n",
    "                                           subsample=0.5,\n",
    "                                           tol=1e-06,\n",
    "                                           learning_rate=0.05,\n",
    "                                           min_samples_split=0.05,\n",
    "                                           criterion='friedman_mse',\n",
    "                                           random_state=67,\n",
    "                                           )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# no model selected default is Random Forest\n",
    "Feature_Selector = BorutaShap(model=feature_model,\n",
    "                              importance_measure='shap',\n",
    "                              classification=True,\n",
    "                              percentile=100,\n",
    "                              pvalue=0.05,\n",
    "                              )\n",
    "\n",
    "Feature_Selector.fit(X=X_train,\n",
    "                     y=y_train,\n",
    "                     n_trials=100,\n",
    "                     sample=False,\n",
    "                     train_or_test='test',\n",
    "                     normalize=True,\n",
    "                     verbose=True,\n",
    "                     random_state=67,\n",
    "                     )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Feature_Selector.plot(which_features='all', figsize=(18, 8))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drops features that were identified by BorutaShap as not important\n",
    "features_to_remove = Feature_Selector.features_to_remove\n",
    "\n",
    "X_train = X_train.drop(columns=features_to_remove)\n",
    "X_test = X_test.drop(columns=features_to_remove)\n",
    "\n",
    "# Print the shape of the new datasets\n",
    "print('Training features:', X_train.shape[1])\n",
    "print('Testing features:', X_test.shape[1])\n",
    "print('Remaining features:', X_train.columns.to_list())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "e3162af6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline model for comparison\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ec666ef5d57426cbe54a191f40e3c74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa442e24",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create and fit baseline model to compare performance\n",
    "baseline_model = DummyClassifier(strategy='most_frequent', random_state=67)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate model accuracy on test data\n",
    "y_baseline_pred = baseline_model.predict(X_test)\n",
    "y_baseline_balanced_accuracy_score = balanced_accuracy_score(y_test, y_baseline_pred)\n",
    "y_baseline_accuracy = accuracy_score(y_test, y_baseline_pred)\n",
    "\n",
    "print('Baseline scores:')\n",
    "print(f\"balanced accuracy score (test): {(round(y_baseline_balanced_accuracy_score, 3) * 100)} percent\")\n",
    "print(f\"accuracy score (test): {(round(y_baseline_accuracy, 3) * 100)} percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ae7f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model pipeline \n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html\n",
    "##### https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "##### https://imbalanced-learn.org/stable/references/over_sampling.html"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdca45c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create list of features for each dtype\n",
    "categorical_features = list(X_train.select_dtypes(include='object'))\n",
    "ordinal_features = X_train.columns[X_train.isin([1, 2, 3, 4, 5, 6]).all()].tolist()\n",
    "boolean_features = X_train.columns[X_train.isin([0, 1]).all()].tolist()\n",
    "\n",
    "# Create list of float features\n",
    "numeric_features = [x for x in X_train.columns if x not in boolean_features]\n",
    "numeric_features = [x for x in numeric_features if x not in categorical_features]\n",
    "numeric_features = [x for x in numeric_features if x not in ordinal_features]\n",
    "\n",
    "print('categorical features:', len(categorical_features), ':', categorical_features)\n",
    "print('ordinal features:', len(ordinal_features), ':', ordinal_features)\n",
    "print('numeric features:', len(numeric_features))\n",
    "print('boolean features:', len(boolean_features))\n",
    "print('total features:', len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0c787e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier removal\n",
    "def IQR_Outliers(X, features):\n",
    "\n",
    "    indices = [x for x in X.index]\n",
    "    out_index_list = []\n",
    "        \n",
    "    for col in features:\n",
    "        # Using nanpercentile instead of percentile because of nan values\n",
    "        Q1 = np.nanpercentile(X[col], 25.)\n",
    "        Q3 = np.nanpercentile(X[col], 75.)\n",
    "        \n",
    "        cut_off = (Q3 - Q1) * 1.5\n",
    "        upper, lower = Q3 + cut_off, Q1 - cut_off\n",
    "                \n",
    "        outliers_index = X[col][(X[col] < lower) | (X[col] > upper)].index.tolist()\n",
    "        outliers = X[col][(X[col] < lower) | (X[col] > upper)].values\n",
    "        \n",
    "        out_index_list.extend(outliers_index)\n",
    "        \n",
    "    # Use set to remove duplicates\n",
    "    out_index_list = list(set(out_index_list))\n",
    "    out_index_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the transformations steps per category\n",
    "num_transform = Pipeline(steps=[('smpl_imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "                                ('iqr_outlier', IQR_Outliers(X_train, numeric_features)),\n",
    "                                ('power_trans', PowerTransformer(method='yeo-johnson', copy=False)),\n",
    "                                ('standard_scaler', StandardScaler()),\n",
    "                               ])\n",
    "\n",
    "cat_transform = Pipeline(steps=[('smpl_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                                ('one_hot_encoder', OneHotEncoder(sparse=False, handle_unknown='ignore')),\n",
    "                               ])\n",
    "\n",
    "ord_transform = Pipeline(steps=[('smpl_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                                ('ordinal_encoder', OrdinalEncoder()),\n",
    "                               ])\n",
    "                                \n",
    "Column_Tranform = ColumnTransformer(transformers=[('numeric_trans', num_transform, numeric_features),\n",
    "                                                  ('categorical_trans', cat_transform, categorical_features),\n",
    "                                                  ('ordinal_trans', ord_transform, ordinal_features),\n",
    "                                                  ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94cf136",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specify tree model used for feature selection\n",
    "feat_model = RandomForestClassifier(n_estimators=400,\n",
    "                                    class_weight='balanced_subsample',\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=67,\n",
    "                                   )\n",
    "\n",
    "# Create feature selection model to be imbedded in the HalvingRandomSearchCV pipeline\n",
    "Feat_Select_Eval = RFE(estimator=feat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca8ac9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specify number of target classes\n",
    "n_classes = y_train.nunique()\n",
    "\n",
    "# Specify HalvingRandomSearchCV halving parameter\n",
    "halving_parameter = 2.0\n",
    "\n",
    "# Specify the HalvingRandomSearchCV minimum/maximun resources\n",
    "max_resource = 1600\n",
    "resource_divisor = 2.0\n",
    "min_resource = int(round((max_resource / resource_divisor), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51cb7e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def random_search():\n",
    "    pipeline1 = Pipeline([\n",
    "    ('col', Column_Tranform),\n",
    "    ('feat', Feat_Select_Eval),\n",
    "    ('smpl', ADASYN(n_neighbors=n_classes, sampling_strategy='not majority', n_jobs=-1, random_state=67)),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "    ])\n",
    "\n",
    "    pipeline2 = Pipeline([\n",
    "    ('col', Column_Tranform),\n",
    "    ('feat', Feat_Select_Eval),\n",
    "    ('smpl', ADASYN(n_neighbors=n_classes, sampling_strategy='not majority', n_jobs=-1, random_state=67)),\n",
    "    ('clf', ExtraTreesClassifier()),\n",
    "    ])\n",
    "    \n",
    "    pipeline3 = Pipeline([\n",
    "    ('col', Column_Tranform),\n",
    "    ('feat', Feat_Select_Eval),\n",
    "    ('smpl', ADASYN(n_neighbors=n_classes, sampling_strategy='not majority', n_jobs=-1, random_state=67)),\n",
    "    ('clf', GradientBoostingClassifier()),\n",
    "    ])\n",
    " \n",
    "    # RandomForestClassifier\n",
    "    parameters1 = {\n",
    "    'feat__n_features_to_select': loguniform(0.40, 1.00),\n",
    "    'feat__step': randint(2, 15),\n",
    "    'clf__criterion': ['gini'],\n",
    "    'clf__max_features': ['sqrt', 'log2', None],\n",
    "    'clf__max_depth': [None],\n",
    "    'clf__max_samples': loguniform(0.60, 1.0),\n",
    "    'clf__min_samples_split': randint(10, 70),\n",
    "    'clf__min_samples_leaf': randint(10, 50),\n",
    "    'clf__min_impurity_decrease': loguniform(1e-07, 1e-03),\n",
    "    'clf__min_weight_fraction_leaf':  loguniform(1e-08, 1e-02),\n",
    "    'clf__ccp_alpha':  loguniform(1e-05, 1e-01),\n",
    "    'clf__bootstrap': [True],\n",
    "    'clf__oob_score': [False],\n",
    "    'clf__warm_start': [False],\n",
    "    'clf__n_jobs': [6],\n",
    "    'clf__random_state': [67],\n",
    "    }\n",
    "\n",
    "    # ExtraTreesClassifier\n",
    "    parameters2 = {\n",
    "    'feat__n_features_to_select': loguniform(0.50, 0.90),\n",
    "    'feat__step': randint(2, 15),\n",
    "    'clf__criterion': ['gini'],\n",
    "    'clf__max_features': ['sqrt', 'log2', None],\n",
    "    'clf__max_depth': [None],\n",
    "    'clf__max_leaf_nodes': [None],\n",
    "    'clf__max_samples': loguniform(0.60, 1.0),\n",
    "    'clf__min_samples_split': randint(10, 60),\n",
    "    'clf__min_samples_leaf': randint(10, 60),\n",
    "    'clf__min_weight_fraction_leaf': loguniform(1e-06, 1e-02),\n",
    "    'clf__min_impurity_decrease': loguniform(1e-09, 1e-05),\n",
    "    'clf__ccp_alpha': loguniform(1e-06, 1e-02),\n",
    "    'clf__bootstrap': [True],\n",
    "    'clf__oob_score': [False],\n",
    "    'clf__warm_start': [False],\n",
    "    'clf__n_jobs': [6],\n",
    "    'clf__random_state': [67],\n",
    "    }\n",
    " \n",
    "    # GradientBoostingClassifier\n",
    "    parameters3 = {\n",
    "    'feat__n_features_to_select': loguniform(0.80, 1.00),\n",
    "    'feat__step': randint(2, 15),\n",
    "    'clf__max_features': ['sqrt', 'log2', None],  \n",
    "    'clf__learning_rate': loguniform(1e-04, 1e-01),\n",
    "    'clf__ccp_alpha': loguniform(1e-07, 1e-03),\n",
    "    'clf__max_depth': randint(5, 20),\n",
    "    'clf__max_leaf_nodes': randint(10, 80),\n",
    "    'clf__min_samples_split': randint(40, 120),\n",
    "    'clf__min_impurity_decrease': loguniform(1e-04, 1e-01),\n",
    "    'clf__min_samples_leaf': randint(10, 70),\n",
    "    'clf__n_iter_no_change': [150, 175, 200, None],\n",
    "    'clf__tol': loguniform(1e-09, 1e-06),\n",
    "    'clf__validation_fraction': loguniform(0.10, 0.30),\n",
    "    'clf__warm_start': [False],\n",
    "    'clf__random_state': [67],\n",
    "    }\n",
    "\n",
    "    pars = [parameters1, parameters2, parameters3]\n",
    "    pips = [pipeline1, pipeline2, pipeline3]\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(pars)):\n",
    "        \n",
    "        rs = HalvingRandomSearchCV(pips[i],\n",
    "                                   pars[i],\n",
    "                                   factor=halving_parameter,\n",
    "                                   resource='clf__n_estimators',\n",
    "                                   n_candidates='exhaust',\n",
    "                                   min_resources=min_resource,\n",
    "                                   max_resources=max_resource,\n",
    "                                   scoring='balanced_accuracy',\n",
    "                                   aggressive_elimination=False,\n",
    "                                   return_train_score=False,\n",
    "                                   refit=True,\n",
    "                                   cv=5,\n",
    "                                   n_jobs=6,\n",
    "                                   verbose=1,\n",
    "                                   random_state=67,\n",
    "                                  )\n",
    "\n",
    "        start = time()\n",
    "        \n",
    "        # Fit models on training data\n",
    "        rs = rs.fit(X_train, y_train)\n",
    "        \n",
    "        # Apply models to test data to determine model performance\n",
    "        y_pred = rs.predict(X_test)\n",
    "        y_pred_prob = rs.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        print(\"Hyperparameter search completed in %.2f minutes\" % ((time() - start)/ 60))\n",
    "        print(' ')\n",
    "        \n",
    "        # storing model results\n",
    "        result.append({\n",
    "        'grid': rs,\n",
    "        'cv results': rs.cv_results_,\n",
    "        'train score': rs.best_score_,\n",
    "        'best params': rs.best_params_, \n",
    "        'best estimator': rs.best_estimator_,\n",
    "        'feature importances': rs.best_estimator_.named_steps['clf'].feature_importances_,\n",
    "        'selected feature count': rs.best_estimator_.named_steps['feat'].n_features_,\n",
    "        'selected features alt': rs.best_estimator_.named_steps['feat'].get_feature_names_out(),\n",
    "        'selected features': rs.best_estimator_.named_steps['feat'].support_,\n",
    "        'test balanced accuracy score': balanced_accuracy_score(y_test, y_pred),\n",
    "        'test accuracy score': accuracy_score(y_test, y_pred),\n",
    "        'test weighted f1 score': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'test classification report': classification_report(y_test, \n",
    "                                                            y_pred, \n",
    "                                                            target_names=['short','deep','inside','outside']),\n",
    "        'cv': rs.cv,\n",
    "        'model #': i + 1\n",
    "        })  \n",
    "\n",
    "    # sorting results by best test score\n",
    "    result = sorted(result, key=operator.itemgetter('test balanced accuracy score'), reverse=True)\n",
    "    \n",
    "    print(' ')\n",
    "    \n",
    "    for element in result:\n",
    "        if element['model #']==1:\n",
    "            print('RandomForest classifier: ')\n",
    "        elif element['model #']==2:\n",
    "            print('ExtraTrees classifier: ')\n",
    "        elif element['model #']==3:\n",
    "            print('GradientBoosting classifier: ')\n",
    "        else:\n",
    "            print('Other: ')\n",
    "            \n",
    "        print('Parameters:         ' + str(element['best params']))\n",
    "        print(' ')\n",
    "        print('Candidate features:', initial_features)\n",
    "        print('')\n",
    "        print(str(element['selected feature count']) + ' features selected during evaluation')\n",
    "        print('Features:  ' + str(element['selected features alt']))\n",
    "        print(' ')\n",
    "        print('Train balanced accuracy score: ' + str(element['train score']))\n",
    "        print('Test balanced accuracy score:  ' + str(element['test balanced accuracy score']))\n",
    "        print('Test accuracy score:           ' + str(element['test accuracy score']))\n",
    "        print('Test weighted f1 score:        ' + str(element['test weighted f1 score']))\n",
    "        print(' ')\n",
    "        print(str(element['test classification report']))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print most significant features per model\n",
    "        f_list = []\n",
    "        total_importance = 0\n",
    "        included_feats = []\n",
    "        \n",
    "        for f in zip(initial_features,\n",
    "                     rs.best_estimator_.named_steps['feat'].get_feature_names_out(),\n",
    "                     rs.best_estimator_.named_steps['clf'].feature_importances_):\n",
    "            f_list.append(f)\n",
    "            total_importance += f[2]\n",
    "\n",
    "        for f in zip(initial_features,\n",
    "                     rs.best_estimator_.named_steps['feat'].get_feature_names_out(),\n",
    "                     rs.best_estimator_.named_steps['clf'].feature_importances_):\n",
    "            if f[2] > .01:\n",
    "                included_feats.append(f[0])\n",
    "        \n",
    "        print('\\n',\"Cumulative Importance =\", total_importance)\n",
    "        \n",
    "        df2 = pd.DataFrame(f_list, columns =['feature','index','importance']).sort_values(by='importance', \n",
    "                                                                                  ascending=False)\n",
    "        df2['cum_sum'] = df2['importance'].cumsum()\n",
    "        print(df2.head(40))\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "\n",
    "    # Save best model as pickle file\n",
    "    joblib.dump(rs.best_params_, 'multiclass_play_classifier_results.pkl', compress = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0014a5ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define start time of this stage in the process\n",
    "start = time_calc.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876aabee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "random_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd6dfe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define end time for process and calculate total time elapsed\n",
    "end = time_calc.time()\n",
    "print(round((end - start)/3600, 2), 'hours to complete hyperparameter tuning process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c9f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "nfl_offensive_play_classification_v1.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
