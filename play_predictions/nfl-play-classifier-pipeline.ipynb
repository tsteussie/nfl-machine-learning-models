{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "warnings.simplefilter('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore',category=ImportWarning)\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import randint, loguniform\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, HalvingRandomSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774348c602e5ca1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of continuous features to be binned into intervals\n",
    "ordinal_features = ['week','remaining_downs']\n",
    "binned_features = ['play_sequence_game','game_seconds_remaining']\n",
    "\n",
    "# Train test split parameters\n",
    "test_holdout_percentage = 0.10\n",
    "\n",
    "# RFE feature selection\n",
    "rfc_estimators = 50\n",
    "rfc_max_features = 'sqrt'\n",
    "\n",
    "# Define list of scalers being used\n",
    "scaler_list = [StandardScaler(), \n",
    "               RobustScaler(), \n",
    "               MinMaxScaler(), \n",
    "               MaxAbsScaler(), \n",
    "               PowerTransformer(),\n",
    "               QuantileTransformer(output_distribution='normal'), \n",
    "               QuantileTransformer(output_distribution='uniform'),\n",
    "               ]\n",
    "\n",
    "# Specify the HalvingRandomSearchCV parameters\n",
    "scoring = 'accuracy'                        \n",
    "n_cross_validation = 3\n",
    "halving_parameter = 5.0\n",
    "max_resource = 1000\n",
    "resource_divisor = 5.0\n",
    "min_resource = int(round((max_resource / resource_divisor), 0))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39c59abc33e5e515"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create timer to calculate total workbook time in hours\n",
    "start_time = time.time()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dc8446203dc414e"
  },
  {
   "cell_type": "markdown",
   "id": "c4f8e047",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## IMPORT PROCESSED NFL-DATA-PY CSV FILE\n",
    "##### https://pypi.org/project/nfl-data-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import csv file from nfl-data-py\n",
    "df = pd.read_csv(r'/Users/ttas2/Documents/Python/nfl-machine-learning-models/output_files/nfl_post_processing_run_pass_classification_data.csv')\n",
    "\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8d686327dce3e32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert binary columns to integers\n",
    "binary_columns = df.columns[df.isin([0,1]).all()].tolist()\n",
    "df[binary_columns] = df[binary_columns].apply(pd.to_numeric, downcast='integer', errors='coerce', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41f7422bcc2cf003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print columns with missing values\n",
    "print(df.columns[df.isnull().any()].tolist())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "629bc39586b0fba6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9bcb28",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Target frequency\n",
    "target_count = df.play_type.value_counts(normalize=True)\n",
    "\n",
    "target_count"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TRAIN TEST SPLIT\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c140526b778c182"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943bbf5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split data into target and feature datasets\n",
    "X, y = df.loc[:, df.columns != 'play_type'], df['play_type']\n",
    "\n",
    "initial_features = X.columns.to_list()\n",
    "\n",
    "# Create train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_holdout_percentage, random_state=67)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cd4c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BASELINE MODEL\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9513e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create and fit baseline model to compare performance\n",
    "baseline_model = DummyClassifier(strategy='most_frequent', random_state=67)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate model accuracy on test data\n",
    "y_baseline_pred = baseline_model.predict(X_test)\n",
    "\n",
    "print(f\"Baseline accuracy: {round(accuracy_score(y_test,y_baseline_pred)*100, 1)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MODEL PIPELINE\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html\n",
    "##### https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html\n",
    "##### https://imbalanced-learn.org/stable/references/over_sampling.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "##### https://xgboost.readthedocs.io/en/stable/parameter.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28eb7c49ea06b6d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create feature type lists for column transform stage of the pipeline\n",
    "categorical_features = list(X_train.select_dtypes(include='object'))\n",
    "boolean_features = X_train.columns[X_train.isin([0, 1]).all()].tolist()\n",
    "numeric_features = list(set(X_train.columns) - set(ordinal_features) - set(categorical_features) - set(boolean_features) - set(binned_features))\n",
    "\n",
    "print('Boolean features:', len(boolean_features)) #, ':', boolean_features)\n",
    "print('Numeric features:', len(numeric_features), ':', numeric_features)\n",
    "print('Ordinal features:', len(ordinal_features), ':', ordinal_features)\n",
    "print('Binned features:', len(binned_features), ':', binned_features)\n",
    "print('Categorical features:', len(categorical_features), ':', categorical_features)\n",
    "print(' ')\n",
    "print(len(initial_features), 'total features')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e95804b4c341fcbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Custom transformer for IQR outlier exclusion\n",
    "class IQRTransformer:\n",
    "    def __init__(self, numerical_cols):\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.lower_bound = None\n",
    "        self.upper_bound = None\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        if isinstance(x, pd.DataFrame):\n",
    "            # Calculate the IQR for each numerical column\n",
    "            q1 = x[self.numerical_cols].quantile(0.25)\n",
    "            q3 = x[self.numerical_cols].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "\n",
    "            # Define the lower and upper bounds for outliers\n",
    "            self.lower_bound = (q1 - 1.5 * iqr).to_dict()\n",
    "            self.upper_bound = (q3 + 1.5 * iqr).to_dict()\n",
    "        else:\n",
    "            # Calculate the IQR for each numerical column\n",
    "            q1 = np.quantile(x[:, :], 0.25, axis=0)\n",
    "            q3 = np.quantile(x[:, :], 0.75, axis=0)\n",
    "            iqr = q3 - q1\n",
    "\n",
    "            # Define the lower and upper bounds for outliers\n",
    "            self.lower_bound = (q1 - 1.5 * iqr).tolist()\n",
    "            self.upper_bound = (q3 + 1.5 * iqr).tolist()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, x, y=None):\n",
    "        if isinstance(x, pd.DataFrame):\n",
    "            # Exclude outliers based on the IQR for each numerical column\n",
    "            x_outlier_removed = x.copy()\n",
    "            for col in self.numerical_cols:\n",
    "                if col in self.lower_bound and col in self.upper_bound:\n",
    "                    x_outlier_removed = x_outlier_removed[\n",
    "                        (x_outlier_removed[col] >= self.lower_bound[col]) & (x_outlier_removed[col] <= self.upper_bound[col])\n",
    "                    ].dropna()\n",
    "        else:\n",
    "            # Exclude outliers based on the IQR for each numerical column\n",
    "            x_outlier_removed = x.copy()\n",
    "            for i, col in enumerate(self.numerical_cols):\n",
    "                if col in self.lower_bound and col in self.upper_bound:\n",
    "                    lower_bound = self.lower_bound[col]\n",
    "                    upper_bound = self.upper_bound[col]\n",
    "                    x_outlier_removed = x_outlier_removed[\n",
    "                        (x_outlier_removed[:, i] >= lower_bound) & (x_outlier_removed[:, i] <= upper_bound)\n",
    "                    ].dropna()\n",
    "\n",
    "        return x_outlier_removed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "716f61f917c8a815"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create custom transformer for selecting a feature scaler: StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler\n",
    "class ScalerSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, scaler=StandardScaler()):\n",
    "        super().__init__()\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self.scaler.fit(x)\n",
    "\n",
    "    def transform(self, x, y=None):\n",
    "        return self.scaler.transform(x)\n",
    "    \n",
    "# scaler and encoder options\n",
    "feature_scaler = ScalerSelector()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cb04ee44508f87e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Specify the transformations per data type\n",
    "num_trans = Pipeline(steps=[('num_imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "                            ('num_outlier', IQRTransformer(numerical_cols=numeric_features)),\n",
    "                            ('num_scaler', feature_scaler),\n",
    "                           ])\n",
    "\n",
    "cat_trans = Pipeline(steps=[('cat_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                            ('cat_encoder', LeaveOneOutEncoder(handle_missing='value', handle_unknown='value', random_state=67)),\n",
    "                            ])\n",
    "\n",
    "ord_trans = Pipeline(steps=[('ord_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                            ('ord_encoder', OrdinalEncoder(categories='auto', handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "                           ])\n",
    "\n",
    "bin_trans = Pipeline(steps=[('bin_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                            ('bin_discretizer', KBinsDiscretizer(subsample=None, random_state=67)),\n",
    "                            ('bin_scaler', feature_scaler),\n",
    "                           ])\n",
    "\n",
    "preprocessing = ColumnTransformer(transformers=[('num_transform', num_trans, numeric_features),\n",
    "                                                ('cat_transform', cat_trans, categorical_features),\n",
    "                                                ('ord_transform', ord_trans, ordinal_features),\n",
    "                                                ('bin_transform', bin_trans, binned_features),\n",
    "                                                ],\n",
    "                                  remainder='passthrough',\n",
    "                                  )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b32bfe8bf6eb824",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the models\n",
    "models = [\n",
    "    ('RandomForest', RandomForestClassifier()),\n",
    "    ('ExtraTrees', ExtraTreesClassifier()),\n",
    "    ('GradientBoosting', GradientBoostingClassifier()),\n",
    "    ('AdaBoost', AdaBoostClassifier()),\n",
    "    ('XGBoost', XGBClassifier()),\n",
    "]\n",
    "\n",
    "# Create and run the pipeline\n",
    "for model_name, model in models:\n",
    "    pipeline = Pipeline([\n",
    "        ('pre', preprocessing),\n",
    "        ('sel', RFE(RandomForestClassifier(n_estimators=rfc_estimators, max_features=rfc_max_features, class_weight='balanced_subsample', n_jobs=6, random_state=67))),\n",
    "        ('ovr', ADASYN(sampling_strategy='not majority', random_state=67)),\n",
    "        ('clf', model)\n",
    "    ])\n",
    " \n",
    "    params = {}\n",
    "\n",
    "    if model_name == 'RandomForest':\n",
    "        params = {\n",
    "            'pre__cat_transform__cat_encoder__sigma': loguniform(0.001, 0.05),\n",
    "            'pre__num_transform__num_scaler': scaler_list,\n",
    "            'pre__bin_transform__bin_discretizer__n_bins': randint(10, 25),\n",
    "            'pre__bin_transform__bin_discretizer__strategy': ['uniform','quantile','kmeans'], \n",
    "            'pre__bin_transform__bin_discretizer__encode': ['ordinal'],\n",
    "            'pre__bin_transform__bin_scaler': scaler_list,\n",
    "            'sel__n_features_to_select': randint(40, 80),\n",
    "            'sel__step': [2],\n",
    "            'ovr__n_neighbors': randint(2, 6),\n",
    "            'clf__bootstrap': [True],\n",
    "            'clf__ccp_alpha': loguniform(1e-05, 1e-00),                                   # Cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting\n",
    "            'clf__criterion': ['gini'],\n",
    "            'clf__max_depth': randint(60, 90),\n",
    "            'clf__max_features': loguniform(0.30, 0.70), \n",
    "            'clf__min_impurity_decrease': loguniform(1e-10, 1e-06),\n",
    "            #'clf__max_samples': loguniform(0.05, 0.40),                                  # Only for bootstrap=True. Including this parameter significantly reduced model performance. \n",
    "            'clf__min_samples_leaf': loguniform(0.05, 0.25),\n",
    "            'clf__min_samples_split': loguniform(0.05, 0.25),\n",
    "            'clf__min_weight_fraction_leaf': loguniform(0.05, 0.25),\n",
    "            'clf__oob_score': [False],                                                    # Only for bootstrap=True\n",
    "            'clf__warm_start': [True],\n",
    "            'clf__n_jobs': [6],\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "\n",
    "    elif model_name == 'ExtraTrees':\n",
    "        params = {\n",
    "            'pre__cat_transform__cat_encoder__sigma': loguniform(0.001, 0.05),\n",
    "            'pre__num_transform__num_scaler': scaler_list,\n",
    "            'pre__bin_transform__bin_discretizer__n_bins': randint(10, 25),\n",
    "            'pre__bin_transform__bin_discretizer__strategy': ['uniform','quantile','kmeans'], \n",
    "            'pre__bin_transform__bin_discretizer__encode': ['ordinal'],\n",
    "            'pre__bin_transform__bin_scaler': scaler_list,\n",
    "            'sel__n_features_to_select': randint(35, 85),\n",
    "            'sel__step': [2],\n",
    "            'ovr__n_neighbors': randint(2, 6), \n",
    "            'clf__bootstrap': [False],\n",
    "            'clf__ccp_alpha': loguniform(1e-08, 1e-03),                                   # Cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting\n",
    "            'clf__criterion': ['entropy'],\n",
    "            'clf__max_depth': randint(50, 85),\n",
    "            'clf__max_features': loguniform(0.20, 0.55),\n",
    "            'clf__max_leaf_nodes': randint(30, 75),\n",
    "            #'clf__max_samples': loguniform(0.10, 0.50),                                  # Only for bootstrap=True\n",
    "            'clf__min_impurity_decrease': loguniform(1e-10, 1e-07),\n",
    "            'clf__min_samples_leaf': loguniform(0.001, 0.10),\n",
    "            'clf__min_samples_split': loguniform(0.001, 0.10),\n",
    "            'clf__min_weight_fraction_leaf': loguniform(0.001, 0.10),\n",
    "            'clf__oob_score': [False],                                                    # Only for bootstrap=True\n",
    "            'clf__warm_start': [False],\n",
    "            'clf__n_jobs': [6],\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "\n",
    "    elif model_name == 'GradientBoosting':\n",
    "        params = {\n",
    "            'pre__cat_transform__cat_encoder__sigma': loguniform(0.001, 0.05),\n",
    "            'pre__num_transform__num_scaler': scaler_list,\n",
    "            'pre__bin_transform__bin_discretizer__n_bins': randint(10, 25),\n",
    "            'pre__bin_transform__bin_discretizer__strategy': ['uniform','quantile','kmeans'], \n",
    "            'pre__bin_transform__bin_discretizer__encode': ['ordinal'],\n",
    "            'pre__bin_transform__bin_scaler': scaler_list,\n",
    "            'sel__n_features_to_select': randint(30, 70),\n",
    "            'sel__step': [2],\n",
    "            'ovr__n_neighbors':randint(2, 6),\n",
    "            'clf__criterion': ['friedman_mse'],\n",
    "            'clf__ccp_alpha': loguniform(1e-10, 1e-06),                                   # Cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting\n",
    "            'clf__learning_rate': loguniform(1e-02, 1e-00),\n",
    "            'clf__loss': ['exponential'],\n",
    "            'clf__max_depth': randint(30, 65),\n",
    "            'clf__max_features': loguniform(0.20, 0.55), \n",
    "            'clf__max_leaf_nodes': randint(50, 80),\n",
    "            'clf__min_weight_fraction_leaf': loguniform(0.30, 0.50),                      # Must be <= 0.5\n",
    "            'clf__min_impurity_decrease': loguniform(1e-10, 1e-07),\n",
    "            'clf__min_samples_leaf': loguniform(0.005, 0.15),\n",
    "            'clf__min_samples_split': loguniform(0.02, 0.25),\n",
    "            'clf__n_iter_no_change': [150],\n",
    "            'clf__tol': loguniform(1e-10, 1e-06),\n",
    "            'clf__validation_fraction': [0.07],\n",
    "            'clf__warm_start': [True],\n",
    "            'clf__subsample': loguniform(0.95, 1.0),\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "\n",
    "    elif model_name == 'AdaBoost':\n",
    "        params = {\n",
    "            'pre__cat_transform__cat_encoder__sigma': loguniform(0.001, 0.05),\n",
    "            'pre__num_transform__num_scaler': scaler_list,\n",
    "            'pre__bin_transform__bin_discretizer__n_bins': randint(10, 25),\n",
    "            'pre__bin_transform__bin_discretizer__strategy': ['uniform','quantile','kmeans'], \n",
    "            'pre__bin_transform__bin_discretizer__encode': ['ordinal'],\n",
    "            'pre__bin_transform__bin_scaler': scaler_list,\n",
    "            'sel__n_features_to_select': randint(5, 40),\n",
    "            'sel__step': [2],\n",
    "            'ovr__n_neighbors': randint(2, 6), \n",
    "            'clf__algorithm': ['SAMME'],\n",
    "            'clf__learning_rate': loguniform(1e-03, 1e-00),\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "\n",
    "    elif model_name == 'XGBoost':\n",
    "        params = {\n",
    "            'pre__cat_transform__cat_encoder__sigma': loguniform(0.001, 0.05),\n",
    "            'pre__num_transform__num_scaler': scaler_list,\n",
    "            'pre__bin_transform__bin_discretizer__n_bins': randint(10, 25),\n",
    "            'pre__bin_transform__bin_discretizer__strategy': ['uniform','quantile','kmeans'], \n",
    "            'pre__bin_transform__bin_discretizer__encode': ['ordinal'],\n",
    "            'pre__bin_transform__bin_scaler': scaler_list,\n",
    "            'sel__n_features_to_select': randint(5, 40),\n",
    "            'sel__step': [2],\n",
    "            'ovr__n_neighbors': randint(2, 6), \n",
    "            'clf__tree_method': ['hist'],                                                      # 'hist' is faster than 'auto'\n",
    "            'clf__booster': ['gbtree'],                                                        # 'gbtree' is faster than 'gblinear'\n",
    "            'clf__eta': [0.3],\n",
    "            'clf__max_depth': [8],\n",
    "            'clf__max_bin': [256],\n",
    "            'clf__grow_policy': ['depthwise','lossguide'],                                     # 'lossguide','depthwise'\n",
    "            'clf__objective': ['binary:hinge'],\n",
    "            'clf__eval_metric': ['logloss'],\n",
    "            'clf__seed': [67],\n",
    "        }\n",
    "    \n",
    "    search = HalvingRandomSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=params,\n",
    "        scoring='accuracy',\n",
    "        factor=halving_parameter,\n",
    "        resource='clf__n_estimators',\n",
    "        n_candidates='exhaust',\n",
    "        min_resources=min_resource,\n",
    "        max_resources=max_resource,\n",
    "        aggressive_elimination=False,\n",
    "        return_train_score=True,\n",
    "        refit=True,\n",
    "        cv=n_cross_validation,\n",
    "        n_jobs=-1,\n",
    "        random_state=67,\n",
    "        verbose=1,\n",
    "        error_score='raise',\n",
    "    )\n",
    "    \n",
    "    start_training_time = time.time()\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print training and test results\n",
    "    print('--------------------------')\n",
    "    print(' ')\n",
    "    end_training_time = time.time()\n",
    "    print(f\"{model_name} runtime:\", round((end_training_time - start_training_time) / 60, 0), 'minutes')\n",
    "    print(f\"{model_name} best training score: {search.best_score_}\")\n",
    "    print(f\"{model_name} best test score: {search.score(X_test, y_test)}\")\n",
    "    print(' ')\n",
    "    print(f\"{model_name} parameters: {search.best_params_}\")\n",
    "    print(' ')\n",
    "\n",
    "    # Print features selected during recursive feature selection (RFE)\n",
    "    selected_features = zip(search.best_estimator_.named_steps['sel'].ranking_, initial_features, search.best_estimator_.named_steps['sel'].support_)\n",
    "    selected_features = [x for x in selected_features if x[2] == True]\n",
    "    selected_features = [x[1] for x in selected_features]\n",
    "    print(f\"{model_name} features: {selected_features}\")\n",
    "    print(' ')\n",
    "    \n",
    "    # Print classification report and confusion matrix\n",
    "    print(classification_report(y_test, search.predict(X_test), target_names=['run','pass'], digits=4))\n",
    "    cm = confusion_matrix(y_test, search.predict(X_test))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['run','pass']).plot(cmap=plt.cm.Blues) \n",
    "    plt.show()\n",
    "    \n",
    "    print(' ')\n",
    "    print('-------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "58cc5161",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate workbook processing time in hours\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print('Total HalvingRandomSearchCV runtime', round(total_time / 3600, 2), 'hours')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ab8b5efee71f3d3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f892f582b7b36c28",
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "name": "nfl_offensive_play_classification_v1.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
