{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ec1286",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np                 # maintain version 1.22.0\n",
    "import time as time_calc\n",
    "from time import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, HalvingRandomSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "\n",
    "from scipy.stats import randint\n",
    "import joblib\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8e047",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import data from nfl-data-py\n",
    "##### https://pypi.org/project/nfl-data-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c775fae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4486, 29)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv with 5-years of nfl play-by-play data (2020-2021)\n",
    "data = pd.read_csv('nfl_post_processing_draft_data.csv')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20845ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['games_pcnt'] = df['games'] / (df['nfl_years'] * 16)\n",
    "df['games_pcnt'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bcbf94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['pfr_player_id','player_name','category','position','side','college','hof','allpro',\n",
    "         'pro_bowls','seasons_started','combine','comb_name','comb_school','nfl_years','games',\n",
    "         'draft','round'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3b7908d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7cb01\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7cb01_level0_col0\" class=\"col_heading level0 col0\" >pick</th>\n",
       "      <th id=\"T_7cb01_level0_col1\" class=\"col_heading level0 col1\" >draft_team</th>\n",
       "      <th id=\"T_7cb01_level0_col2\" class=\"col_heading level0 col2\" >age</th>\n",
       "      <th id=\"T_7cb01_level0_col3\" class=\"col_heading level0 col3\" >comb_pos</th>\n",
       "      <th id=\"T_7cb01_level0_col4\" class=\"col_heading level0 col4\" >comb_ht</th>\n",
       "      <th id=\"T_7cb01_level0_col5\" class=\"col_heading level0 col5\" >comb_wt</th>\n",
       "      <th id=\"T_7cb01_level0_col6\" class=\"col_heading level0 col6\" >comb_forty</th>\n",
       "      <th id=\"T_7cb01_level0_col7\" class=\"col_heading level0 col7\" >comb_bench</th>\n",
       "      <th id=\"T_7cb01_level0_col8\" class=\"col_heading level0 col8\" >comb_vert</th>\n",
       "      <th id=\"T_7cb01_level0_col9\" class=\"col_heading level0 col9\" >comb_broad</th>\n",
       "      <th id=\"T_7cb01_level0_col10\" class=\"col_heading level0 col10\" >comb_cone</th>\n",
       "      <th id=\"T_7cb01_level0_col11\" class=\"col_heading level0 col11\" >comb_shut</th>\n",
       "      <th id=\"T_7cb01_level0_col12\" class=\"col_heading level0 col12\" >games_pcnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7cb01_level0_row0\" class=\"row_heading level0 row0\" >1749</th>\n",
       "      <td id=\"T_7cb01_row0_col0\" class=\"data row0 col0\" >22</td>\n",
       "      <td id=\"T_7cb01_row0_col1\" class=\"data row0 col1\" >MIN</td>\n",
       "      <td id=\"T_7cb01_row0_col2\" class=\"data row0 col2\" >21.000000</td>\n",
       "      <td id=\"T_7cb01_row0_col3\" class=\"data row0 col3\" >WR</td>\n",
       "      <td id=\"T_7cb01_row0_col4\" class=\"data row0 col4\" >61.000000</td>\n",
       "      <td id=\"T_7cb01_row0_col5\" class=\"data row0 col5\" >192.000000</td>\n",
       "      <td id=\"T_7cb01_row0_col6\" class=\"data row0 col6\" >4.390000</td>\n",
       "      <td id=\"T_7cb01_row0_col7\" class=\"data row0 col7\" >19.000000</td>\n",
       "      <td id=\"T_7cb01_row0_col8\" class=\"data row0 col8\" >36.000000</td>\n",
       "      <td id=\"T_7cb01_row0_col9\" class=\"data row0 col9\" >122.000000</td>\n",
       "      <td id=\"T_7cb01_row0_col10\" class=\"data row0 col10\" >6.940000</td>\n",
       "      <td id=\"T_7cb01_row0_col11\" class=\"data row0 col11\" >4.200000</td>\n",
       "      <td id=\"T_7cb01_row0_col12\" class=\"data row0 col12\" >0.585938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7cb01_level0_row1\" class=\"row_heading level0 row1\" >3397</th>\n",
       "      <td id=\"T_7cb01_row1_col0\" class=\"data row1 col0\" >199</td>\n",
       "      <td id=\"T_7cb01_row1_col1\" class=\"data row1 col1\" >CIN</td>\n",
       "      <td id=\"T_7cb01_row1_col2\" class=\"data row1 col2\" >22.000000</td>\n",
       "      <td id=\"T_7cb01_row1_col3\" class=\"data row1 col3\" >WR</td>\n",
       "      <td id=\"T_7cb01_row1_col4\" class=\"data row1 col4\" >75.000000</td>\n",
       "      <td id=\"T_7cb01_row1_col5\" class=\"data row1 col5\" >205.000000</td>\n",
       "      <td id=\"T_7cb01_row1_col6\" class=\"data row1 col6\" >4.470000</td>\n",
       "      <td id=\"T_7cb01_row1_col7\" class=\"data row1 col7\" >14.000000</td>\n",
       "      <td id=\"T_7cb01_row1_col8\" class=\"data row1 col8\" >31.500000</td>\n",
       "      <td id=\"T_7cb01_row1_col9\" class=\"data row1 col9\" >119.000000</td>\n",
       "      <td id=\"T_7cb01_row1_col10\" class=\"data row1 col10\" >6.940000</td>\n",
       "      <td id=\"T_7cb01_row1_col11\" class=\"data row1 col11\" >4.200000</td>\n",
       "      <td id=\"T_7cb01_row1_col12\" class=\"data row1 col12\" >0.796875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7febe3904c70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert binary columns to integers\n",
    "binary_columns = df.columns[df.isin([0,1]).all()].tolist()\n",
    "df[binary_columns] = df[binary_columns].apply(pd.to_numeric, downcast='integer', errors='coerce', axis=1)\n",
    "\n",
    "df.sample(2).style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130ba51",
   "metadata": {},
   "source": [
    "## Statistical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b84162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pick</th>\n",
       "      <td>4486.0</td>\n",
       "      <td>112.541239</td>\n",
       "      <td>70.377165</td>\n",
       "      <td>1.00</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>260.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>4486.0</td>\n",
       "      <td>22.452073</td>\n",
       "      <td>0.914298</td>\n",
       "      <td>20.00</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>29.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comb_ht</th>\n",
       "      <td>4486.0</td>\n",
       "      <td>72.177441</td>\n",
       "      <td>6.780501</td>\n",
       "      <td>0.00</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>81.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comb_wt</th>\n",
       "      <td>4486.0</td>\n",
       "      <td>245.440259</td>\n",
       "      <td>45.349512</td>\n",
       "      <td>155.00</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>375.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comb_forty</th>\n",
       "      <td>4486.0</td>\n",
       "      <td>4.745265</td>\n",
       "      <td>0.297886</td>\n",
       "      <td>4.22</td>\n",
       "      <td>4.510000</td>\n",
       "      <td>4.660000</td>\n",
       "      <td>4.930000</td>\n",
       "      <td>5.9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comb_bench</th>\n",
       "      <td>4486.0</td>\n",
       "      <td>21.077686</td>\n",
       "      <td>5.915244</td>\n",
       "      <td>2.00</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>49.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comb_vert</th>\n",
       "      <td>4486.0</td>\n",
       "      <td>33.336781</td>\n",
       "      <td>3.900197</td>\n",
       "      <td>19.50</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>46.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comb_broad</th>\n",
       "      <td>4486.0</td>\n",
       "      <td>115.396790</td>\n",
       "      <td>8.770500</td>\n",
       "      <td>82.00</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>147.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comb_cone</th>\n",
       "      <td>4486.0</td>\n",
       "      <td>7.223107</td>\n",
       "      <td>0.364993</td>\n",
       "      <td>6.44</td>\n",
       "      <td>6.940000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>7.445000</td>\n",
       "      <td>9.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comb_shut</th>\n",
       "      <td>4486.0</td>\n",
       "      <td>4.357882</td>\n",
       "      <td>0.233378</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.190000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.490000</td>\n",
       "      <td>5.3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>games_pcnt</th>\n",
       "      <td>4486.0</td>\n",
       "      <td>0.630243</td>\n",
       "      <td>0.282933</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>1.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count        mean        std     min         25%         50%  \\\n",
       "pick        4486.0  112.541239  70.377165    1.00   52.000000  106.000000   \n",
       "age         4486.0   22.452073   0.914298   20.00   22.000000   22.000000   \n",
       "comb_ht     4486.0   72.177441   6.780501    0.00   72.000000   74.000000   \n",
       "comb_wt     4486.0  245.440259  45.349512  155.00  207.000000  237.000000   \n",
       "comb_forty  4486.0    4.745265   0.297886    4.22    4.510000    4.660000   \n",
       "comb_bench  4486.0   21.077686   5.915244    2.00   16.000000   21.000000   \n",
       "comb_vert   4486.0   33.336781   3.900197   19.50   31.000000   33.500000   \n",
       "comb_broad  4486.0  115.396790   8.770500   82.00  110.000000  117.000000   \n",
       "comb_cone   4486.0    7.223107   0.364993    6.44    6.940000    7.100000   \n",
       "comb_shut   4486.0    4.357882   0.233378    3.73    4.190000    4.300000   \n",
       "games_pcnt  4486.0    0.630243   0.282933    0.00    0.453125    0.708333   \n",
       "\n",
       "                   75%       max  \n",
       "pick        168.000000  260.0000  \n",
       "age          23.000000   29.0000  \n",
       "comb_ht      76.000000   81.0000  \n",
       "comb_wt     288.000000  375.0000  \n",
       "comb_forty    4.930000    5.9900  \n",
       "comb_bench   25.000000   49.0000  \n",
       "comb_vert    36.000000   46.0000  \n",
       "comb_broad  122.000000  147.0000  \n",
       "comb_cone     7.445000    9.0000  \n",
       "comb_shut     4.490000    5.3800  \n",
       "games_pcnt    0.854167    1.0625  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b49f4b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train test split data\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4943bbf5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 features before criteria evaluation\n"
     ]
    }
   ],
   "source": [
    "# split data into target and feature datasets\n",
    "X, y = df.loc[:, df.columns != 'games_pcnt'], df['games_pcnt']\n",
    "\n",
    "# initial_features = df.drop(['play_type'], axis=1)\n",
    "initial_features = X.columns.to_list()\n",
    "\n",
    "# Create train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=67)\n",
    "\n",
    "print(X_train.shape[1], 'features before criteria evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cd4c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline model\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e9513e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline mean squared error: 0.0851743661\n",
      "Baseline R-squared: -0.0022207881\n"
     ]
    }
   ],
   "source": [
    "# Create and fit baseline model to compare performance\n",
    "baseline_model = DummyRegressor(strategy='mean')\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate model accuracy on test data\n",
    "y_baseline_pred = baseline_model.predict(X_test)\n",
    "print(f\"Baseline mean squared error: {round(mean_squared_error(y_test,y_baseline_pred),10)}\")\n",
    "print(f\"Baseline R-squared: {round(r2_score(y_test,y_baseline_pred),10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5630d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model pipeline \n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html\n",
    "##### https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "##### https://imbalanced-learn.org/stable/references/over_sampling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f6723b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical features: 2\n",
      "ordinal features: 0\n",
      "numeric features: 10\n",
      "boolean features: 0\n",
      "total features: 12\n"
     ]
    }
   ],
   "source": [
    "# Create list of features for each dtype\n",
    "categorical_features = list(X_train.select_dtypes(include='object')) # 'drive_start','surface'\n",
    "ordinal_features = X_train.columns[X_train.isin([1, 2, 3, 4, 5, 6]).all()].tolist() # 'qtr', 'down'\n",
    "boolean_features = X_train.columns[X_train.isin([0, 1]).all()].tolist()\n",
    "\n",
    "# Create list of float features\n",
    "numeric_features = [x for x in X_train.columns if x not in boolean_features]\n",
    "numeric_features = [x for x in numeric_features if x not in categorical_features]\n",
    "numeric_features = [x for x in numeric_features if x not in ordinal_features]\n",
    "\n",
    "print('categorical features:', len(categorical_features))\n",
    "print('ordinal features:', len(ordinal_features))\n",
    "print('numeric features:', len(numeric_features))\n",
    "print('boolean features:', len(boolean_features))\n",
    "print('total features:', len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4ff9bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier removal\n",
    "def IQR_Outliers(X, features):\n",
    "\n",
    "    indices = [x for x in X.index]\n",
    "    out_index_list = []\n",
    "        \n",
    "    for col in features:\n",
    "        # Using nanpercentile instead of percentile because of nan values\n",
    "        Q1 = np.nanpercentile(X[col], 10.)\n",
    "        Q3 = np.nanpercentile(X[col], 90.)\n",
    "        \n",
    "        cut_off = (Q3 - Q1) * 1.5\n",
    "        upper, lower = Q3 + cut_off, Q1 - cut_off\n",
    "                \n",
    "        outliers_index = X[col][(X[col] < lower) | (X[col] > upper)].index.tolist()\n",
    "        outliers = X[col][(X[col] < lower) | (X[col] > upper)].values\n",
    "        \n",
    "        out_index_list.extend(outliers_index)\n",
    "        \n",
    "    # Use set to remove duplicates\n",
    "    out_index_list = list(set(out_index_list))\n",
    "    out_index_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58cc5161",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the transformations per data type\n",
    "num_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "                            ('iqr_outlier', IQR_Outliers(X_train, numeric_features)),\n",
    "                            ('power_trans', PowerTransformer(method='yeo-johnson', copy=False)),\n",
    "                            ('std_scaler', StandardScaler()),\n",
    "                           ])\n",
    "\n",
    "cat_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                            ('onehot_encoder', OneHotEncoder(sparse=False, handle_unknown='ignore')),\n",
    "                           ])\n",
    "\n",
    "ord_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                            ('ordinal_encoder', OrdinalEncoder()),\n",
    "                           ])\n",
    "                                \n",
    "Column_Tranform = ColumnTransformer(transformers=[('numeric_transform', num_trans, numeric_features),\n",
    "                                                  ('categorical_transform', cat_trans, categorical_features),\n",
    "                                                  ('ordinal_transfrom', ord_trans, ordinal_features),\n",
    "                                                  ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac59c5a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specify tree model used for feature selection\n",
    "feature_model = RandomForestRegressor(n_estimators=500,\n",
    "                                      criterion='squared_error',\n",
    "                                      n_jobs=-1,\n",
    "                                      random_state=67,\n",
    "                                     )\n",
    "\n",
    "# feature selection model used in the HalvingRandomSearchCV pipeline\n",
    "Feature_Selector = RFE(estimator=feature_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd01cef5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specify number of target classes\n",
    "n_classes = y_train.nunique()\n",
    "\n",
    "# Specify HalvingRandomSearchCV halving parameter\n",
    "halving_parameter = 2.0\n",
    "\n",
    "# Specify the HalvingRandomSearchCV minimum/maximun resources\n",
    "max_resource = 3000\n",
    "resource_divisor = 2.0\n",
    "min_resource = int(round((max_resource / resource_divisor), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f9a1f5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def random_search():\n",
    "    \n",
    "    pipeline1 = Pipeline([\n",
    "    ('col', Column_Tranform),\n",
    "    ('feat', Feature_Selector),\n",
    "    ('reg', RandomForestRegressor()),\n",
    "    ])\n",
    "    \n",
    "    pipeline2 = Pipeline([\n",
    "    ('col', Column_Tranform),\n",
    "    ('feat', Feature_Selector),\n",
    "    ('reg', ExtraTreesRegressor()),\n",
    "    ])\n",
    "    \n",
    "    pipeline3 = Pipeline([\n",
    "    ('col', Column_Tranform),\n",
    "    ('feat', Feature_Selector),\n",
    "    ('reg', GradientBoostingRegressor()),\n",
    "    ])\n",
    "    \n",
    "    # RandomForestRegressor\n",
    "    parameters1 = {\n",
    "    'feat__n_features_to_select': loguniform(0.20, 1.0),\n",
    "    'feat__step': randint(2, 10),\n",
    "    'reg__criterion': ['squared_error'],\n",
    "    'reg__max_features': ['sqrt', 'log2', None],\n",
    "    'reg__max_depth': [None],\n",
    "    'reg__min_samples_split': randint(2, 100),\n",
    "    'reg__min_samples_leaf': randint(2, 100),\n",
    "    'reg__min_impurity_decrease': loguniform(1e-09, 1e-01),\n",
    "    'reg__min_weight_fraction_leaf':  loguniform(1e-08, 1e-01),\n",
    "    'reg__ccp_alpha':  loguniform(1e-09, 1e-01),\n",
    "    'reg__bootstrap': [True, False],\n",
    "    'reg__oob_score': [False],\n",
    "    'reg__warm_start': [True, False],\n",
    "    'reg__n_jobs': [6],\n",
    "    'reg__random_state': [67],\n",
    "    }\n",
    "    \n",
    "    # ExtraTreesRegressor\n",
    "    parameters2 = {\n",
    "    'feat__n_features_to_select': loguniform(0.20, 1.0),\n",
    "    'feat__step': randint(2, 10),\n",
    "    'reg__criterion': ['squared_error'],\n",
    "    'reg__max_depth': [None],\n",
    "    'reg__max_features': ['sqrt', 'log2', None],\n",
    "    'reg__max_leaf_nodes': [None],\n",
    "    'reg__max_samples': [None],\n",
    "    'reg__min_samples_split': randint(2, 100),\n",
    "    'reg__min_samples_leaf': randint(2, 100),\n",
    "    'reg__min_weight_fraction_leaf': loguniform(1e-09, 1e-02),\n",
    "    'reg__min_impurity_decrease': loguniform(1e-09, 1e-02),\n",
    "    'reg__ccp_alpha': loguniform(1e-09, 1e-02),\n",
    "    'reg__bootstrap': [True, False],\n",
    "    'reg__oob_score': [False],\n",
    "    'reg__warm_start': [True, False],\n",
    "    'reg__n_jobs': [6],\n",
    "    'reg__random_state': [67],\n",
    "    }\n",
    "            \n",
    "    # GradientBoostingRegressor\n",
    "    parameters3 = {\n",
    "    'feat__n_features_to_select': loguniform(0.20, 1.00),\n",
    "    'feat__step': randint(2, 10),\n",
    "    'reg__loss': ['squared_error', 'absolute_error', 'huber'],\n",
    "    'reg__max_features': ['sqrt', 'log2', None],  \n",
    "    'reg__learning_rate': loguniform(1e-09, 1e-01),\n",
    "    'reg__subsample': loguniform(0.01, 1.0),\n",
    "    'reg__criterion': ['friedman_mse'],\n",
    "    'reg__alpha': loguniform(0.01, 1.0),\n",
    "    'reg__ccp_alpha': loguniform(1e-09, 1e-02),\n",
    "    'reg__max_depth': randint(5, 30),\n",
    "    'reg__max_leaf_nodes': randint(1, 120),\n",
    "    'reg__min_samples_split': randint(10, 120), \n",
    "    'reg__min_impurity_decrease': loguniform(1e-09, 1e-02),\n",
    "    'reg__min_samples_leaf': randint(2, 100),\n",
    "    'reg__n_iter_no_change': [100, 125, 150, 175, 200, None],\n",
    "    'reg__tol': loguniform(1e-09, 1e-02),\n",
    "    'reg__validation_fraction': loguniform(0.05, 0.30),\n",
    "    'reg__warm_start': [True, False],\n",
    "    'reg__random_state': [67],\n",
    "    }\n",
    "\n",
    "    pars = [parameters1, parameters2, parameters3]\n",
    "    pips = [pipeline1, pipeline2, pipeline3]\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(pars)):\n",
    "        \n",
    "        rs = HalvingRandomSearchCV(pips[i],\n",
    "                                   pars[i],\n",
    "                                   factor=halving_parameter,\n",
    "                                   resource='reg__n_estimators',\n",
    "                                   n_candidates='exhaust',\n",
    "                                   min_resources=min_resource,\n",
    "                                   max_resources=max_resource,\n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   aggressive_elimination=False,\n",
    "                                   return_train_score=True,\n",
    "                                   refit=True,\n",
    "                                   cv=5,\n",
    "                                   n_jobs=6,\n",
    "                                   verbose=1,\n",
    "                                   random_state=67,\n",
    "                                   error_score='raise',\n",
    "                                  )\n",
    "        \n",
    "        start = time()\n",
    "        \n",
    "        # Fit models on training data\n",
    "        rs = rs.fit(X_train, y_train)\n",
    "        \n",
    "        # Apply models to test data to determine model performance\n",
    "        y_pred = rs.predict(X_test)\n",
    "\n",
    "        print(\"HalvingRandomSearchCV required %.2f minutes to complete search\" % ((time() - start)/ 60))\n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "        \n",
    "        # storing model results\n",
    "        result.append({\n",
    "        'grid': rs,\n",
    "        'cv results': rs.cv_results_,\n",
    "        'train score': rs.best_score_,\n",
    "        'best params': rs.best_params_, \n",
    "        'best estimator': rs.best_estimator_,\n",
    "        'feature importances': rs.best_estimator_.named_steps['reg'].feature_importances_,\n",
    "        'selected feature count': rs.best_estimator_.named_steps['feat'].n_features_,\n",
    "        'selected features alt': rs.best_estimator_.named_steps['feat'].get_feature_names_out(),\n",
    "        'test score': mean_squared_error(y_test, y_pred),\n",
    "        'test score alt': r2_score(y_test, y_pred),\n",
    "        'cv': rs.cv,\n",
    "        'model #': i + 1\n",
    "        })\n",
    "\n",
    "    # sorting results by best test score\n",
    "    result = sorted(result, key=operator.itemgetter('test score'), reverse=False)\n",
    "    \n",
    "    print('Best Models:')\n",
    "    print(' ')\n",
    "    for element in result:\n",
    "        if element['model #']==1:\n",
    "            print('RandomForest Regressor: ')\n",
    "        elif element['model #']==2:\n",
    "            print('ExtraTrees Regressor: ')\n",
    "        elif element['model #']==3:\n",
    "            print('GradientBoosting Regressor: ')\n",
    "        else:\n",
    "            print('Other Regressor: ')  \n",
    "        print('Parameters:  ' + str(element['best params']))\n",
    "        print(' ')\n",
    "        print('Candidate features:', initial_features)\n",
    "        print('')\n",
    "        print(str(element['selected feature count']) + ' features selected during evaluation')\n",
    "        print('Features:  ' + str(element['selected features alt']))\n",
    "        print(' ')\n",
    "        print('Train mean squared error: ' + str(element['train score']))\n",
    "        print('Test mean squared error:  ' + str(element['test score']))\n",
    "        print('Test r2 error:            ' + str(element['test score alt']))\n",
    "\n",
    "        # Print most significant features per model\n",
    "        f_list = []\n",
    "        total_importance = 0\n",
    "        included_feats = []\n",
    "        \n",
    "        for f in zip(initial_features, \n",
    "                     rs.best_estimator_.named_steps['feat'].get_feature_names_out(),\n",
    "                     rs.best_estimator_.named_steps['reg'].feature_importances_):\n",
    "            f_list.append(f)\n",
    "            total_importance += f[2]\n",
    "            \n",
    "        # Print the name and gini importance of each feature\n",
    "        for f in zip(initial_features, \n",
    "                     rs.best_estimator_.named_steps['feat'].get_feature_names_out(),\n",
    "                     rs.best_estimator_.named_steps['reg'].feature_importances_):\n",
    "            if f[2] > .01:\n",
    "                included_feats.append(f[0])\n",
    "                \n",
    "        print('\\n',\"Cumulative Importance =\", total_importance)\n",
    "        \n",
    "        df2 = pd.DataFrame(f_list, columns=['feat','index','importance']).sort_values(by='importance',\n",
    "                                                                                      ascending=False)\n",
    "        df2['cum_sum'] = df2['importance'].cumsum()\n",
    "        print(df2.head(20))\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "        \n",
    "    # Save best model as pickle file\n",
    "    joblib.dump(rs.best_params_, 'nfl_draft_prediction_results.pkl', compress = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e2a914",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define start time of this stage in the process\n",
    "start = time_calc.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7e2a726",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 1500\n",
      "max_resources_: 3000\n",
      "aggressive_elimination: False\n",
      "factor: 2.0\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 2\n",
      "n_resources: 1500\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1\n",
      "n_resources: 3000\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "HalvingRandomSearchCV required 2.16 minutes to complete search\n",
      " \n",
      " \n",
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 1500\n",
      "max_resources_: 3000\n",
      "aggressive_elimination: False\n",
      "factor: 2.0\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 2\n",
      "n_resources: 1500\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1\n",
      "n_resources: 3000\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "HalvingRandomSearchCV required 1.96 minutes to complete search\n",
      " \n",
      " \n",
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 1500\n",
      "max_resources_: 3000\n",
      "aggressive_elimination: False\n",
      "factor: 2.0\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 2\n",
      "n_resources: 1500\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1\n",
      "n_resources: 3000\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "HalvingRandomSearchCV required 1.45 minutes to complete search\n",
      " \n",
      " \n",
      "Best Models:\n",
      " \n",
      "ExtraTrees Regressor: \n",
      "Parameters:  {'feat__n_features_to_select': 0.8970655378663026, 'feat__step': 5, 'reg__bootstrap': True, 'reg__ccp_alpha': 2.1294485265202466e-09, 'reg__criterion': 'squared_error', 'reg__max_depth': None, 'reg__max_features': None, 'reg__max_leaf_nodes': None, 'reg__max_samples': None, 'reg__min_impurity_decrease': 1.8673236044140839e-06, 'reg__min_samples_leaf': 44, 'reg__min_samples_split': 60, 'reg__min_weight_fraction_leaf': 5.118688118147405e-06, 'reg__n_jobs': 6, 'reg__oob_score': False, 'reg__random_state': 67, 'reg__warm_start': False, 'reg__n_estimators': 3000}\n",
      " \n",
      "Candidate features: ['pick', 'draft_team', 'age', 'comb_pos', 'comb_ht', 'comb_wt', 'comb_forty', 'comb_bench', 'comb_vert', 'comb_broad', 'comb_cone', 'comb_shut']\n",
      "\n",
      "56 features selected during evaluation\n",
      "Features:  ['x0' 'x1' 'x2' 'x3' 'x4' 'x5' 'x6' 'x7' 'x8' 'x9' 'x10' 'x11' 'x12' 'x13'\n",
      " 'x14' 'x15' 'x16' 'x17' 'x18' 'x19' 'x20' 'x21' 'x22' 'x23' 'x24' 'x25'\n",
      " 'x26' 'x27' 'x28' 'x29' 'x30' 'x31' 'x32' 'x34' 'x35' 'x36' 'x37' 'x38'\n",
      " 'x39' 'x40' 'x41' 'x42' 'x43' 'x45' 'x46' 'x47' 'x50' 'x51' 'x54' 'x55'\n",
      " 'x56' 'x58' 'x59' 'x60' 'x61' 'x62']\n",
      " \n",
      "Train mean squared error: -0.061267734704459086\n",
      "Test mean squared error:  0.06363206559789869\n",
      "Test r2 error:            0.25126089174805055\n",
      "\n",
      " Cumulative Importance = 0.903944655599698\n",
      "          feat index  importance   cum_sum\n",
      "0         pick    x0    0.461269  0.461269\n",
      "1   draft_team    x1    0.068223  0.529492\n",
      "6   comb_forty    x6    0.060641  0.590134\n",
      "4      comb_ht    x4    0.055748  0.645882\n",
      "7   comb_bench    x7    0.052611  0.698492\n",
      "8    comb_vert    x8    0.048155  0.746647\n",
      "9   comb_broad    x9    0.047935  0.794582\n",
      "3     comb_pos    x3    0.044698  0.839280\n",
      "5      comb_wt    x5    0.044359  0.883638\n",
      "2          age    x2    0.020197  0.903835\n",
      "11   comb_shut   x11    0.000110  0.903945\n",
      "10   comb_cone   x10    0.000000  0.903945\n",
      " \n",
      " \n",
      " \n",
      "RandomForest Regressor: \n",
      "Parameters:  {'feat__n_features_to_select': 0.8970655378663026, 'feat__step': 5, 'reg__bootstrap': True, 'reg__ccp_alpha': 2.372260913931848e-09, 'reg__criterion': 'squared_error', 'reg__max_depth': None, 'reg__max_features': None, 'reg__min_impurity_decrease': 5.4769234501792595e-06, 'reg__min_samples_leaf': 44, 'reg__min_samples_split': 60, 'reg__min_weight_fraction_leaf': 5.118688118147405e-05, 'reg__n_jobs': 6, 'reg__oob_score': False, 'reg__random_state': 67, 'reg__warm_start': False, 'reg__n_estimators': 3000}\n",
      " \n",
      "Candidate features: ['pick', 'draft_team', 'age', 'comb_pos', 'comb_ht', 'comb_wt', 'comb_forty', 'comb_bench', 'comb_vert', 'comb_broad', 'comb_cone', 'comb_shut']\n",
      "\n",
      "56 features selected during evaluation\n",
      "Features:  ['x0' 'x1' 'x2' 'x3' 'x4' 'x5' 'x6' 'x7' 'x8' 'x9' 'x10' 'x11' 'x12' 'x13'\n",
      " 'x14' 'x15' 'x16' 'x17' 'x18' 'x19' 'x20' 'x21' 'x22' 'x23' 'x24' 'x25'\n",
      " 'x26' 'x27' 'x28' 'x29' 'x30' 'x31' 'x32' 'x34' 'x35' 'x36' 'x37' 'x38'\n",
      " 'x39' 'x40' 'x41' 'x42' 'x43' 'x45' 'x46' 'x47' 'x50' 'x51' 'x54' 'x55'\n",
      " 'x56' 'x58' 'x59' 'x60' 'x61' 'x62']\n",
      " \n",
      "Train mean squared error: -0.0624792809650603\n",
      "Test mean squared error:  0.06463894238873977\n",
      "Test r2 error:            0.23941327964540604\n",
      "\n",
      " Cumulative Importance = 0.903944655599698\n",
      "          feat index  importance   cum_sum\n",
      "0         pick    x0    0.461269  0.461269\n",
      "1   draft_team    x1    0.068223  0.529492\n",
      "6   comb_forty    x6    0.060641  0.590134\n",
      "4      comb_ht    x4    0.055748  0.645882\n",
      "7   comb_bench    x7    0.052611  0.698492\n",
      "8    comb_vert    x8    0.048155  0.746647\n",
      "9   comb_broad    x9    0.047935  0.794582\n",
      "3     comb_pos    x3    0.044698  0.839280\n",
      "5      comb_wt    x5    0.044359  0.883638\n",
      "2          age    x2    0.020197  0.903835\n",
      "11   comb_shut   x11    0.000110  0.903945\n",
      "10   comb_cone   x10    0.000000  0.903945\n",
      " \n",
      " \n",
      " \n",
      "GradientBoosting Regressor: \n",
      "Parameters:  {'feat__n_features_to_select': 0.9952273407781347, 'feat__step': 4, 'reg__alpha': 0.010525148466232696, 'reg__ccp_alpha': 7.364098164498887e-07, 'reg__criterion': 'friedman_mse', 'reg__learning_rate': 1.986257784093469e-06, 'reg__loss': 'squared_error', 'reg__max_depth': 19, 'reg__max_features': 'log2', 'reg__max_leaf_nodes': 23, 'reg__min_impurity_decrease': 5.100452736375567e-06, 'reg__min_samples_leaf': 21, 'reg__min_samples_split': 59, 'reg__n_iter_no_change': 150, 'reg__random_state': 67, 'reg__subsample': 0.14172504514833267, 'reg__tol': 2.579584251515049e-06, 'reg__validation_fraction': 0.08095318021664964, 'reg__warm_start': True, 'reg__n_estimators': 3000}\n",
      " \n",
      "Candidate features: ['pick', 'draft_team', 'age', 'comb_pos', 'comb_ht', 'comb_wt', 'comb_forty', 'comb_bench', 'comb_vert', 'comb_broad', 'comb_cone', 'comb_shut']\n",
      "\n",
      "62 features selected during evaluation\n",
      "Features:  ['x0' 'x1' 'x2' 'x3' 'x4' 'x5' 'x6' 'x7' 'x8' 'x9' 'x10' 'x11' 'x12' 'x13'\n",
      " 'x14' 'x15' 'x16' 'x17' 'x18' 'x19' 'x20' 'x21' 'x22' 'x23' 'x24' 'x25'\n",
      " 'x26' 'x27' 'x28' 'x29' 'x30' 'x31' 'x32' 'x33' 'x34' 'x35' 'x36' 'x37'\n",
      " 'x38' 'x39' 'x40' 'x41' 'x42' 'x43' 'x44' 'x45' 'x46' 'x47' 'x48' 'x49'\n",
      " 'x50' 'x51' 'x52' 'x53' 'x54' 'x55' 'x56' 'x58' 'x59' 'x60' 'x61' 'x62']\n",
      " \n",
      "Train mean squared error: -0.07959651867014601\n",
      "Test mean squared error:  0.08516177843135093\n",
      "Test r2 error:            -0.0020726726486381963\n",
      "\n",
      " Cumulative Importance = 0.903944655599698\n",
      "          feat index  importance   cum_sum\n",
      "0         pick    x0    0.461269  0.461269\n",
      "1   draft_team    x1    0.068223  0.529492\n",
      "6   comb_forty    x6    0.060641  0.590134\n",
      "4      comb_ht    x4    0.055748  0.645882\n",
      "7   comb_bench    x7    0.052611  0.698492\n",
      "8    comb_vert    x8    0.048155  0.746647\n",
      "9   comb_broad    x9    0.047935  0.794582\n",
      "3     comb_pos    x3    0.044698  0.839280\n",
      "5      comb_wt    x5    0.044359  0.883638\n",
      "2          age    x2    0.020197  0.903835\n",
      "11   comb_shut   x11    0.000110  0.903945\n",
      "10   comb_cone   x10    0.000000  0.903945\n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "random_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25296d0a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09 hours to complete hyperparameter tuning process\n"
     ]
    }
   ],
   "source": [
    "# Define end time for process and calculate total time elapsed\n",
    "end = time_calc.time()\n",
    "print(round((end - start)/3600, 2), 'hours to complete hyperparameter tuning process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a30ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "nfl_offensive_play_classification_v1.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
