{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "warnings.simplefilter('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore',category=ImportWarning)\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import randint, loguniform\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import LeaveOneOutEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, HalvingRandomSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774348c602e5ca1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train test split parameters\n",
    "test_holdout_percentage = 0.25\n",
    "\n",
    "# Leave One Out Encoder Sigma value - 0.04 is the top performing value.\n",
    "sigma = 0.05                                                        # Tested parameters: 0.04, 0.05, 0.10, 0.30, 0.60\n",
    "\n",
    "# Feature scaler\n",
    "feature_scaler = StandardScaler()                                   # Tested parameters: MinMaxScaler(), StandardScaler(), MaxAbsScaler(), RobustScaler()\n",
    "\n",
    "# HalvingRandomSearchCV parameters\n",
    "scoring = 'f1_weighted'\n",
    "n_cross_validation = 3\n",
    "\n",
    "# Specify the HalvingRandomSearchCV parameters\n",
    "halving_parameter = 2.0\n",
    "max_resource = 1000\n",
    "resource_divisor = 2.0\n",
    "min_resource = int(round((max_resource / resource_divisor), 0))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39c59abc33e5e515"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create timer to calculate total workbook time in hours\n",
    "start_time = time.time()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dc8446203dc414e"
  },
  {
   "cell_type": "markdown",
   "id": "c4f8e047",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## IMPORT PROCESSED NFL-DATA-PY CSV FILE\n",
    "##### https://pypi.org/project/nfl-data-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import csv file from nfl-data-py\n",
    "df = pd.read_csv(r'/Users/ttas2/Documents/Python/nfl-machine-learning-models/output_files/nfl_post_processing_multiclass_play_classification_data.csv')\n",
    "\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8d686327dce3e32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print columns with missing values\n",
    "print(df.columns[df.isnull().any()].tolist())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "629bc39586b0fba6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert binary columns to integers\n",
    "binary_columns = df.columns[df.isin([0,1]).all()].tolist()\n",
    "df[binary_columns] = df[binary_columns].apply(pd.to_numeric, downcast='integer', errors='coerce', axis=1)\n",
    "\n",
    "df.sample(2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41f7422bcc2cf003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9bcb28",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Target frequency\n",
    "target_count = df.play_type.value_counts(normalize=True)\n",
    "\n",
    "target_count"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TRAIN TEST SPLIT\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c140526b778c182"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674bc94c9111c14",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data into target and feature datasets\n",
    "X, y = df.loc[:, df.columns != 'play_type'], df['play_type']\n",
    "\n",
    "initial_features = X.columns.to_list()\n",
    "\n",
    "# Print target labels\n",
    "y.head(3)"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "# Encode target labels\n",
    "enc = OneHotEncoder().fit(y)\n",
    "y = enc.transform(y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2e3d94e3685b05c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57cfba84ba8fefd3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Create train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_holdout_percentage, random_state=67)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "4943bbf5"
  },
  {
   "cell_type": "markdown",
   "id": "4b6cd4c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BASELINE MODEL\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9513e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create and fit baseline model to compare performance\n",
    "baseline_model = DummyClassifier(strategy='most_frequent', random_state=67)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate model accuracy on test data\n",
    "y_baseline_pred = baseline_model.predict(X_test)\n",
    "\n",
    "print(f\"Baseline f1 score: {round(f1_score(y_test,y_baseline_pred, average='weighted')*100, 1)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MODEL PIPELINE\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html\n",
    "##### https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html\n",
    "##### https://imbalanced-learn.org/stable/references/over_sampling.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "##### https://xgboost.readthedocs.io/en/stable/parameter.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28eb7c49ea06b6d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create feature type lists for column transform stage of the pipeline\n",
    "ordinal_features = X_train.columns[X_train.isin([1,2,3,4,5]).all()].tolist()\n",
    "categorical_features = list(X_train.select_dtypes(include='object'))\n",
    "boolean_features = X_train.columns[X_train.isin([0, 1]).all()].tolist()\n",
    "\n",
    "# define numeric features as remaining features not in ordinal categorical or boolean lists\n",
    "numeric_features = list(set(X_train.columns) - set(ordinal_features) - set(categorical_features) - set(boolean_features))\n",
    "\n",
    "#print('categorical features:', len(categorical_features), ':', categorical_features)\n",
    "print('ordinal features:', len(ordinal_features), ':', ordinal_features)\n",
    "print(' ')\n",
    "print('boolean features:', len(boolean_features), ':', boolean_features)\n",
    "print(' ')\n",
    "print('numeric features:', len(numeric_features), ':', numeric_features)\n",
    "print(' ')\n",
    "print('categorical features:', len(categorical_features), ':', categorical_features)\n",
    "print(' ')\n",
    "print('feature count:', len(initial_features))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e95804b4c341fcbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Custom transformer for IQR outlier exclusion\n",
    "class IQRTransformer:\n",
    "    def __init__(self, numerical_cols):\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.lower_bound = None\n",
    "        self.upper_bound = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Calculate the IQR for each numerical column\n",
    "            q1 = X[self.numerical_cols].quantile(0.25)\n",
    "            q3 = X[self.numerical_cols].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "\n",
    "            # Define the lower and upper bounds for outliers\n",
    "            self.lower_bound = (q1 - 1.5 * iqr).to_dict()\n",
    "            self.upper_bound = (q3 + 1.5 * iqr).to_dict()\n",
    "        else:\n",
    "            # Calculate the IQR for each numerical column\n",
    "            q1 = np.quantile(X[:, :], 0.25, axis=0)\n",
    "            q3 = np.quantile(X[:, :], 0.75, axis=0)\n",
    "            iqr = q3 - q1\n",
    "\n",
    "            # Define the lower and upper bounds for outliers\n",
    "            self.lower_bound = (q1 - 1.5 * iqr).tolist()\n",
    "            self.upper_bound = (q3 + 1.5 * iqr).tolist()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Exclude outliers based on the IQR for each numerical column\n",
    "            x_outlier_removed = X.copy()\n",
    "            for col in self.numerical_cols:\n",
    "                if col in self.lower_bound and col in self.upper_bound:\n",
    "                    x_outlier_removed = x_outlier_removed[\n",
    "                        (x_outlier_removed[col] >= self.lower_bound[col]) & (x_outlier_removed[col] <= self.upper_bound[col])\n",
    "                    ].dropna()\n",
    "        else:\n",
    "            # Exclude outliers based on the IQR for each numerical column\n",
    "            x_outlier_removed = X.copy()\n",
    "            for i, col in enumerate(self.numerical_cols):\n",
    "                if col in self.lower_bound and col in self.upper_bound:\n",
    "                    lower_bound = self.lower_bound[col]\n",
    "                    upper_bound = self.upper_bound[col]\n",
    "                    x_outlier_removed = x_outlier_removed[\n",
    "                        (x_outlier_removed[:, i] >= lower_bound) & (x_outlier_removed[:, i] <= upper_bound)\n",
    "                    ].dropna()\n",
    "\n",
    "        return x_outlier_removed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "716f61f917c8a815"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify the transformations per data type\n",
    "num_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "                            ('iqr_outlier', IQRTransformer(numerical_cols=numeric_features)),\n",
    "                            ('scaler', feature_scaler),\n",
    "                           ])\n",
    "\n",
    "cat_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                            ('cat_encoder', LeaveOneOutEncoder(handle_missing='value', handle_unknown='value', sigma=sigma, random_state=67)),\n",
    "                            ])\n",
    "\n",
    "ord_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                            ('ordinal_encoder', LeaveOneOutEncoder(handle_missing='value', handle_unknown='value', sigma=sigma, random_state=67)),\n",
    "                           ])\n",
    "\n",
    "preprocessing = ColumnTransformer(transformers=[('numeric_transform', num_trans, numeric_features),\n",
    "                                                ('categorical_transform', cat_trans, categorical_features),\n",
    "                                                ('ordinal_transform', ord_trans, ordinal_features),\n",
    "                                                ],\n",
    "                                     remainder='passthrough',\n",
    "                                    )\n",
    "\n",
    "# Define the models\n",
    "models = [\n",
    "    ('RandomForest', RandomForestClassifier()),\n",
    "    ('ExtraTrees', ExtraTreesClassifier()),\n",
    "    #('GradientBoosting', GradientBoostingClassifier()),\n",
    "    #('AdaBoost', AdaBoostClassifier()),\n",
    "    ('XGBoost', XGBClassifier()),\n",
    "]\n",
    "\n",
    "# Create and run the pipeline\n",
    "for model_name, model in models:\n",
    "    pipeline = Pipeline([\n",
    "        ('pre', preprocessing),\n",
    "        ('select', SelectKBest()),\n",
    "        ('smpl', ADASYN(sampling_strategy='not majority', random_state=67)),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    if model_name == 'RandomForest':\n",
    "        params = {\n",
    "            'select__k': randint(20, 60),\n",
    "            'smpl__n_neighbors': randint(4, 8),                             # Only for sampling_strategy='not majority'\n",
    "            'clf__bootstrap': [True],\n",
    "            'clf__ccp_alpha': loguniform(1e-06, 1e-01),                     # cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting\n",
    "            'clf__criterion': ['gini','entropy'],\n",
    "            'clf__max_depth': randint(5, 30),\n",
    "            'clf__max_features': loguniform(0.10, 0.35), \n",
    "            'clf__min_impurity_decrease': loguniform(1e-09, 1e-04),\n",
    "            'clf__max_samples': loguniform(0.02, 0.49),                     # Only for bootstrap=True\n",
    "            'clf__min_samples_leaf': loguniform(0.005, 0.20),\n",
    "            'clf__min_samples_split': loguniform(0.005, 0.20),\n",
    "            'clf__min_weight_fraction_leaf': loguniform(0.005, 0.20),\n",
    "            'clf__oob_score': [True, False],                                # Only for bootstrap=True\n",
    "            'clf__warm_start': [True, False],\n",
    "            'clf__n_jobs': [6],\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "    \n",
    "    elif model_name == 'Extra Trees':\n",
    "        params = {\n",
    "            'select__k': randint(20, 60),\n",
    "            'smpl__n_neighbors': randint(4, 8),                       # Only for sampling_strategy='not majority\n",
    "            'clf__bootstrap': [True, False],\n",
    "            'clf__ccp_alpha': loguniform(1e-06, 1e-01),               # cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting\n",
    "            'clf__criterion': ['gini','entropy'],\n",
    "            'clf__max_depth': randint(5, 80),\n",
    "            'clf__max_features': loguniform(0.50, 0.95),\n",
    "            'clf__max_leaf_nodes': randint(20, 70),\n",
    "            #'clf__max_samples': loguniform(0.10, 0.50),               # Only for bootstrap=True\n",
    "            'clf__min_impurity_decrease': loguniform(1e-05, 1e-01),\n",
    "            'clf__min_samples_leaf': loguniform(0.05, 0.30),\n",
    "            'clf__min_samples_split': loguniform(0.005, 0.15),\n",
    "            'clf__min_weight_fraction_leaf': loguniform(0.05, 0.25),\n",
    "            'clf__oob_score': [False],                                # Only for bootstrap=True\n",
    "            'clf__warm_start': [True, False],\n",
    "            'clf__n_jobs': [6],\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "    \n",
    "    elif model_name == 'GradientBoosting':\n",
    "        params = {\n",
    "            'select__k': randint(20, 60),\n",
    "            'smpl__n_neighbors': randint(4, 8),                       # Only for sampling_strategy='not majority\n",
    "            'clf__criterion': ['friedman_mse'],\n",
    "            'clf__ccp_alpha': loguniform(1e-06, 1e-01),  # cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting\n",
    "            'clf__learning_rate': loguniform(1e-05, 1e-00),\n",
    "            #'clf__loss': ['log_loss'],                                 # Not available for multiclass    \n",
    "            'clf__max_depth': randint(25, 60),\n",
    "            'clf__max_features': loguniform(0.45, 0.85), \n",
    "            'clf__max_leaf_nodes': randint(20, 50),\n",
    "            'clf__min_weight_fraction_leaf': loguniform(0.30, 0.50),   # Must be <= 0.5\n",
    "            'clf__min_impurity_decrease': loguniform(1e-08, 1e-04),\n",
    "            'clf__min_samples_leaf': loguniform(0.01, 0.25),\n",
    "            'clf__min_samples_split': loguniform(0.10, 0.35),\n",
    "            'clf__n_iter_no_change': [200],\n",
    "            'clf__tol': loguniform(1e-08, 1e-03),\n",
    "            'clf__validation_fraction': loguniform(0.05, 0.15),\n",
    "            'clf__warm_start': [True, False],\n",
    "            'clf__subsample': loguniform(0.65, 1.0),\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "        \n",
    "    elif model_name == 'AdaBoost':\n",
    "        params = {\n",
    "            'select__k': randint(20, 60),\n",
    "            'smpl__n_neighbors': randint(4, 8),                       # Only for sampling_strategy='not majority\n",
    "            'clf__algorithm': ['SAMME','SAMME.R'],\n",
    "            'clf__learning_rate': loguniform(1e-08, 1e-01),\n",
    "            'clf__random_state': [67],\n",
    "        }\n",
    "    \n",
    "    elif model_name == 'XGBoost':\n",
    "        params = {\n",
    "            'select__k': randint(20, 60),\n",
    "            'smpl__n_neighbors': randint(4, 8),                       # Only for sampling_strategy='not majority\n",
    "            'clf__booster': ['gbtree','dart'],\n",
    "            'clf__max_depth': [6],\n",
    "            'clf__grow_policy': ['depthwise','lossguide'],\n",
    "            'clf__objective': ['multi:softprob'],\n",
    "            'clf__eval_metric': ['auc'],\n",
    "            'clf__seed': [67],\n",
    "        }\n",
    "\n",
    "    search = HalvingRandomSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=params,\n",
    "        scoring=scoring,\n",
    "        factor=halving_parameter,\n",
    "        resource='clf__n_estimators',\n",
    "        n_candidates='exhaust',\n",
    "        min_resources=min_resource,\n",
    "        max_resources=max_resource,\n",
    "        aggressive_elimination=False,\n",
    "        return_train_score=True,\n",
    "        refit=True,\n",
    "        cv=n_cross_validation,\n",
    "        n_jobs=6,\n",
    "        error_score='raise',\n",
    "        random_state=67,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Best performance for {model_name}: {search.best_score_}\")\n",
    "    print(f\"Best parameters: {search.best_params_}\")\n",
    "\n",
    "    print(\"\\n\")\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "58cc5161"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate workbook processing time in hours\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print('Total HalvingRandomSearchCV runtime:', round(total_time / 3600, 2), 'hours')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7e4b6574bc08686"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8b4ac4275dcfbe6"
  }
 ],
 "metadata": {
  "colab": {
   "name": "nfl_offensive_play_classification_v1.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
