{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "warnings.simplefilter('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore',category=ImportWarning)\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from scipy.stats import randint\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, HalvingRandomSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "import joblib\n",
    "import operator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774348c602e5ca1e"
  },
  {
   "cell_type": "markdown",
   "id": "c4f8e047",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import data from nfl-data-py\n",
    "##### https://pypi.org/project/nfl-data-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import csv file from nfl-data-py\n",
    "df = pd.read_csv(r'/Users/ttas2/Documents/Python/nfl-machine-learning-models/output_files/nfl_post_processing_run_pass_classification_data.csv')\n",
    "\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8d686327dce3e32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print columns with missing values\n",
    "print(df.columns[df.isnull().any()].tolist())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "629bc39586b0fba6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert binary columns to integers\n",
    "binary_columns = df.columns[df.isin([0,1]).all()].tolist()\n",
    "df[binary_columns] = df[binary_columns].apply(pd.to_numeric, downcast='integer', errors='coerce', axis=1)\n",
    "\n",
    "df.sample(2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41f7422bcc2cf003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9bcb28",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Target frequency\n",
    "target_count = df.play_type.value_counts(normalize=True)\n",
    "target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['play_type'] = np.where(df['play_type'] == 'pass', 1, 0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91112ef9f7a1ba3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train test split data\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c140526b778c182"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943bbf5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split data into target and feature datasets\n",
    "X, y = df.loc[:, df.columns != 'play_type'], df['play_type']\n",
    "\n",
    "initial_features = X.columns.to_list()\n",
    "\n",
    "# Create train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=67)\n",
    "\n",
    "print(X_train.shape[1], 'features before feature selection')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encode categorical features\n",
    "##### https://contrib.scikit-learn.org/category_encoders/leaveoneout.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0f3ad4dbdd2f444"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create list of features for each dtype\n",
    "categorical_feat = list(X_train.select_dtypes(include='object'))\n",
    "\n",
    "# One hot encoding of categorical features\n",
    "# encoder = ce.OneHotEncoder(return_df=True, cols=categorical_feat, use_cat_names=True)\n",
    "\n",
    "# Encoded column represents mean response over all rows for this category, providing one-column representation while avoiding direct response leakage\n",
    "encoder = ce.LeaveOneOutEncoder(return_df=True, cols=categorical_feat)\n",
    "\n",
    "X_train = encoder.fit_transform(X_train, y_train)\n",
    "X_test = encoder.transform(X_test)\n",
    "\n",
    "print(X_train.shape[1], 'features after categorical encoding')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d33d4e0b73750e26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BorutaShap Feature selection\n",
    "##### https://pypi.org/project/BorutaShap/\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65fe86e88f378c61"
  },
  {
   "cell_type": "raw",
   "source": [
    "feature_model = XGBClassifier()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1f39b74adc722be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify feature selection model\n",
    "feature_model = GradientBoostingClassifier(loss='log_loss',\n",
    "                                           n_estimators=200,\n",
    "                                           max_depth=50,\n",
    "                                           max_features=0.5,\n",
    "                                           max_leaf_nodes=50,\n",
    "                                           subsample=0.5,\n",
    "                                           tol=1e-06,\n",
    "                                           learning_rate=0.04,\n",
    "                                           min_samples_split=0.05,\n",
    "                                           criterion='friedman_mse',\n",
    "                                           random_state=67,\n",
    "                                           )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30f498bda3ae0723"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 298/300 [20:31:46<03:21, 100.63s/it]"
     ]
    }
   ],
   "source": [
    "# no model selected default is Random Forest\n",
    "Feature_Selector = BorutaShap(model=feature_model,\n",
    "                              importance_measure='shap',\n",
    "                              classification=True,\n",
    "                              )\n",
    "\n",
    "Feature_Selector.fit(X=X_train,\n",
    "                     y=y_train,\n",
    "                     n_trials=200,\n",
    "                     sample=False,\n",
    "                     random_state=67,\n",
    "                     )"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4e7034bfcd562178"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Feature_Selector.plot(which_features='all', figsize=(18, 8))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "535115047848b1cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drops features that were identified by BorutaShap as not important\n",
    "features_to_remove = Feature_Selector.features_to_remove\n",
    "\n",
    "X_train = X_train.drop(columns=features_to_remove)\n",
    "X_test = X_test.drop(columns=features_to_remove)\n",
    "\n",
    "# Print the shape of the new datasets\n",
    "print('Training features:', X_train.shape[1])\n",
    "print('Testing features:', X_test.shape[1])\n",
    "print('Remaining features:', X_train.columns.to_list())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afa5c66c26a42f87"
  },
  {
   "cell_type": "markdown",
   "id": "4b6cd4c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline model\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9513e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create and fit baseline model to compare performance\n",
    "baseline_model = DummyClassifier(strategy='most_frequent', random_state=67)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate model accuracy on test data\n",
    "y_baseline_pred = baseline_model.predict(X_test)\n",
    "accuracy_score(y_test, y_baseline_pred)\n",
    "print(f\"Baseline test accuracy: {(round(accuracy_score(y_test,y_baseline_pred),3)*100)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model pipeline\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html\n",
    "##### https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html\n",
    "##### https://imbalanced-learn.org/stable/references/over_sampling.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "##### https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "##### https://xgboost.readthedocs.io/en/stable/parameter.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28eb7c49ea06b6d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create feature type lists for column transform stage of the pipeline\n",
    "ordinal_features = X_train.columns[X_train.isin([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]).all()].tolist()\n",
    "categorical_features = list(X_train.select_dtypes(include='object'))\n",
    "boolean_features = X_train.columns[X_train.isin([0, 1]).all()].tolist()\n",
    "\n",
    "numeric_features = [x for x in X_train.columns if x not in ordinal_features]\n",
    "numeric_features = [x for x in numeric_features if x not in categorical_features]\n",
    "numeric_features = [x for x in numeric_features if x not in boolean_features]\n",
    "\n",
    "# print('categorical features:', len(categorical_features), ':', categorical_features)\n",
    "print('ordinal features:', len(ordinal_features), ':', ordinal_features)\n",
    "print('boolean features:', len(boolean_features), ':', boolean_features)\n",
    "print('numeric features:', len(numeric_features), ':', numeric_features)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e95804b4c341fcbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ff9bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier removal\n",
    "def IQR_Outliers(x, features):\n",
    "\n",
    "    out_index_list = []\n",
    "        \n",
    "    for col in features:\n",
    "        q1 = np.nanpercentile(x[col], 25.)\n",
    "        q3 = np.nanpercentile(x[col], 75.)\n",
    "        \n",
    "        cut_off = (q3 - q1) * 1.5\n",
    "        upper, lower = q3 + cut_off, q1 - cut_off\n",
    "                \n",
    "        outliers_index = x[col][(x[col] < lower) | (x[col] > upper)].index.tolist()\n",
    "        out_index_list.extend(outliers_index)\n",
    "        \n",
    "    # Remove duplicates\n",
    "    list(set(out_index_list)).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc5161",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the transformations per data type\n",
    "num_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "                            ('iqr_outlier', IQR_Outliers(X_train, numeric_features)),\n",
    "                            ('power_trans', PowerTransformer(method='yeo-johnson', copy=False)),\n",
    "                            ('std_scaler', StandardScaler()),\n",
    "                           ])\n",
    "\n",
    "cat_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                            ('onehot_encoder', OneHotEncoder(sparse=False, handle_unknown='raise')),\n",
    "                           ])\n",
    "\n",
    "ord_trans = Pipeline(steps=[('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                            ('ordinal_encoder', OrdinalEncoder()),\n",
    "                            ('std_scaler', StandardScaler()),\n",
    "                           ])\n",
    "                                \n",
    "Column_Transform = ColumnTransformer(transformers=[('numeric_transform', num_trans, numeric_features),\n",
    "                                                  ('categorical_transform', cat_trans, categorical_features),\n",
    "                                                  ('ordinal_transform', ord_trans, ordinal_features),\n",
    "                                                  ],\n",
    "                                    remainder='passthrough',\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17593a5b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specify number of target classes\n",
    "n_classes = y_train.nunique()\n",
    "\n",
    "# Specify HalvingRandomSearchCV halving_parameter (aka factor)\n",
    "halving_parameter = 2.0\n",
    "\n",
    "# Specify the HalvingRandomSearchCV minimum/maximum resources\n",
    "max_resource = 2500\n",
    "resource_divisor = 2.0\n",
    "min_resource = int(round((max_resource / resource_divisor), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3074a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def random_search():\n",
    "    \n",
    "    pipeline1 = Pipeline([\n",
    "    ('col', Column_Transform),\n",
    "    ('smpl', ADASYN(n_neighbors=n_classes, sampling_strategy='not majority', random_state=67)),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "    ])\n",
    "    \n",
    "    pipeline2 = Pipeline([\n",
    "    ('col', Column_Transform),\n",
    "    ('smpl', ADASYN(n_neighbors=n_classes, sampling_strategy='not majority', random_state=67)),\n",
    "    ('clf', ExtraTreesClassifier()),\n",
    "    ])\n",
    "    \n",
    "    pipeline3 = Pipeline([\n",
    "    ('col', Column_Transform),\n",
    "    ('smpl', ADASYN(n_neighbors=n_classes, sampling_strategy='not majority', random_state=67)),\n",
    "    ('clf', GradientBoostingClassifier()),\n",
    "    ])\n",
    "\n",
    "    # RandomForestClassifier\n",
    "    parameters1 = {\n",
    "    'clf__criterion': ['gini','entropy'],\n",
    "    'clf__max_features': loguniform(0.10, 0.75),\n",
    "    'clf__max_depth': randint(5, 40),\n",
    "    'clf__min_samples_split': loguniform(0.10, 0.60),\n",
    "    'clf__min_samples_leaf': loguniform(0.10, 0.60),\n",
    "    'clf__min_impurity_decrease': loguniform(1e-09, 1e-03),\n",
    "    'clf__min_weight_fraction_leaf':  loguniform(0.15, 0.70),\n",
    "    'clf__ccp_alpha':  loguniform(1e-08, 1e-03),\n",
    "    'clf__bootstrap': [True, False],\n",
    "    'clf__oob_score': [False],\n",
    "    'clf__warm_start': [True, False],\n",
    "    'clf__n_jobs': [6],\n",
    "    'clf__random_state': [67],\n",
    "    }\n",
    "    \n",
    "    # ExtraTreesClassifier\n",
    "    parameters2 = {\n",
    "    'clf__criterion': ['gini'],\n",
    "    'clf__max_depth': randint(10, 50),\n",
    "    'clf__max_features': loguniform(0.20, 0.95),\n",
    "    'clf__max_leaf_nodes': [None],\n",
    "    'clf__max_samples': [None], #loguniform(0.35, 0.75),\n",
    "    'clf__min_samples_split': loguniform(0.10, 0.50),\n",
    "    'clf__min_samples_leaf': loguniform(0.10, 0.50),\n",
    "    'clf__min_weight_fraction_leaf': loguniform(0.10, 0.60),\n",
    "    'clf__min_impurity_decrease': loguniform(1e-09, 1e-03),\n",
    "    'clf__ccp_alpha': loguniform(1e-08, 1e-02),\n",
    "    'clf__bootstrap': [True, False],\n",
    "    'clf__oob_score': [False],\n",
    "    'clf__warm_start': [True, False],\n",
    "    'clf__n_jobs': [6],\n",
    "    'clf__random_state': [67],\n",
    "    }\n",
    "            \n",
    "    # GradientBoostingClassifier\n",
    "    parameters3 = {\n",
    "    'clf__loss': ['log_loss'],\n",
    "    'clf__max_features': loguniform(0.10, 0.50),\n",
    "    'clf__learning_rate': loguniform(1e-07, 1e-02),\n",
    "    'clf__ccp_alpha': loguniform(1e-07, 1e-02),\n",
    "    'clf__max_depth': randint(10, 40),\n",
    "    'clf__max_leaf_nodes': randint(2, 20),\n",
    "    'clf__min_samples_split': loguniform(0.25, 0.60),\n",
    "    'clf__min_impurity_decrease': loguniform(1e-08, 1e-02),\n",
    "    'clf__min_samples_leaf': loguniform(0.25, 0.50),\n",
    "    'clf__n_iter_no_change': [150, 200, None],\n",
    "    'clf__tol': loguniform(1e-08, 1e-04),\n",
    "    'clf__validation_fraction': loguniform(0.05, 0.15),\n",
    "    'clf__warm_start': [False],\n",
    "    'clf__random_state': [67],\n",
    "    }\n",
    "\n",
    "    pars = [parameters1, parameters2, parameters3]\n",
    "    pips = [pipeline1, pipeline2, pipeline3]\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(pars)):\n",
    "        \n",
    "        rs = HalvingRandomSearchCV(pips[i],\n",
    "                                   pars[i],\n",
    "                                   factor=halving_parameter,\n",
    "                                   resource='clf__n_estimators',\n",
    "                                   n_candidates='exhaust',\n",
    "                                   min_resources=min_resource,\n",
    "                                   max_resources=max_resource,\n",
    "                                   scoring='balanced_accuracy',\n",
    "                                   aggressive_elimination=False,\n",
    "                                   return_train_score=True,\n",
    "                                   refit=True,\n",
    "                                   cv=5,\n",
    "                                   n_jobs=6,\n",
    "                                   verbose=1,\n",
    "                                   random_state=67,\n",
    "                                   error_score='raise',\n",
    "                                  )\n",
    "\n",
    "        # Fit models on training data\n",
    "        rs = rs.fit(X_train, y_train)\n",
    "        \n",
    "        # Apply models to test data to determine model performance\n",
    "        y_pred = rs.predict(X_test)\n",
    "        y_pred_prob = rs.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "        \n",
    "        # storing model results\n",
    "        result.append({\n",
    "        'grid': rs,\n",
    "        'cv results': rs.cv_results_,\n",
    "        'best train balanced accuracy score': rs.best_score_,\n",
    "        'best params': rs.best_params_, \n",
    "        'best estimator': rs.best_estimator_,\n",
    "        'feature importance': rs.best_estimator_.named_steps['clf'].feature_importances_,\n",
    "        'best test balanced accuracy score': balanced_accuracy_score(y_test, y_pred),\n",
    "        'best test roc auc score': roc_auc_score(y_test, y_pred_prob),\n",
    "        #'test classification report': classification_report(y_test, y_pred, target_names=['pass','run']),\n",
    "        'cv': rs.cv,\n",
    "        'model #': i + 1\n",
    "        })\n",
    "\n",
    "    # sorting results by best test score\n",
    "    result = sorted(result, key=operator.itemgetter('best test balanced accuracy score'), reverse=True)\n",
    "    \n",
    "    print('Best Models:')\n",
    "    print(' ')\n",
    "    for element in result:\n",
    "        if element['model #']==1:\n",
    "            print('Random Forest classifier: ')\n",
    "        elif element['model #']==2:\n",
    "            print('ExtraTrees classifier: ')\n",
    "        elif element['model #']==3:\n",
    "            print('GradientBoosting classifier: ')\n",
    "        else:\n",
    "            print('Other: ')  \n",
    "        print('Parameters:  ' + str(element['best params']))\n",
    "        print('Train balanced accuracy score:  ' + str(element['best train balanced accuracy score']))\n",
    "        print('Test balanced accuracy score:   ' + str(element['best test balanced accuracy score']))\n",
    "        print('Test roc auc score:   ' + str(element['best test roc auc score']))\n",
    "        print('-----------------------')\n",
    "        print(' ')\n",
    "        print(' ')\n",
    "\n",
    "    # Save best model as pickle file\n",
    "    joblib.dump(rs.best_params_, r'/Users/ttas2/Documents/Python/nfl-machine-learning-models/output_files/run_pass_classifier_results.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2a726",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout --no-display\n",
    "random_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = joblib.load(r'/Users/ttas2/Documents/Python/nfl-machine-learning-models/output_files/run_pass_classifier_results.pkl')\n",
    "best_model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "645562634f287123"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "84e730b18dc73635"
  }
 ],
 "metadata": {
  "colab": {
   "name": "nfl_offensive_play_classification_v1.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
